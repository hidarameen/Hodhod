"""
AI Processing Pipeline
Unified 4-stage AI processing system
Stage 1: Preprocessing (Entity extraction, sentiment analysis)
Stage 2: Rule Engine (Entity replacement, context neutralization)
Stage 3: AI Summarization (Enhanced prompts with rules)
Stage 4: Postprocessing (Validation, verification, formatting)
"""
import asyncio
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
from utils.error_handler import ErrorLogger
from utils.database import db
from services.ai_preprocessing import preprocessing_engine, PreprocessingResult
from services.ai_rule_engine import rule_engine, RuleEngineResult
from services.ai_postprocessing import postprocessing_engine, PostprocessingResult

error_logger = ErrorLogger("ai_pipeline")

@dataclass
class PipelineStageResult:
    """Result from a single pipeline stage"""
    stage_name: str
    input_text: str
    output_text: str
    processing_time: float
    success: bool
    details: Dict[str, Any] = field(default_factory=dict)
    error: Optional[str] = None

@dataclass
class PipelineResult:
    """Complete pipeline processing result"""
    original_text: str
    final_text: str
    stages: List[PipelineStageResult]
    preprocessing: Optional[PreprocessingResult] = None
    rule_engine: Optional[RuleEngineResult] = None
    postprocessing: Optional[PostprocessingResult] = None
    total_time: float = 0.0
    quality_score: float = 1.0
    success: bool = True
    rules_applied_count: int = 0
    entities_replaced: Dict[str, List[Tuple[str, str]]] = field(default_factory=dict)
    warnings: List[str] = field(default_factory=list)
    errors: List[str] = field(default_factory=list)
    extracted_fields: Dict[str, Any] = field(default_factory=dict)

class AIPipeline:
    """
    Main AI Processing Pipeline
    Orchestrates all 4 stages of AI text processing
    """
    
    def __init__(self):
        self.preprocessing = preprocessing_engine
        self.rule_engine = rule_engine
        self.postprocessing = postprocessing_engine
        self.ai_manager = None
        error_logger.log_info("[Pipeline] AI Processing Pipeline initialized")
    
    def set_ai_manager(self, ai_manager):
        """Set the AI manager for text generation"""
        self.ai_manager = ai_manager
    
    async def process(
        self,
        text: str,
        task_id: int,
        provider: str,
        model: str,
        system_prompt: Optional[str] = None,
        custom_rules: Optional[List[Dict]] = None,
        config: Optional[Dict[str, Any]] = None,
        video_source_info: Optional[Dict[str, str]] = None,
        fields_to_extract: Optional[Any] = None,
        serial_number: Optional[int] = None
    ) -> PipelineResult:
        """
        Main pipeline processing function
        Executes all 4 stages in order
        """
        start_time = datetime.now()
        config = config or await self._get_config(task_id)
        
        # If fields_to_extract is True, get them from the template
        if fields_to_extract is True:
            template = await db.get_task_publishing_template(task_id)
            fields_to_extract = template.get("fields", []) if template else []
        
        if not text or not text.strip():
            return self._empty_result(text)
        
        stages = []
        warnings = []
        errors = []
        current_text = text.strip()
        
        error_logger.log_info(f"[Pipeline] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        error_logger.log_info(f"[Pipeline] ğŸš€ STARTING PROCESSING | Task: {task_id} | Text length: {len(current_text)}")
        error_logger.log_info(f"[Pipeline] ğŸ“¥ INPUT TEXT: {current_text[:200]}...")
        error_logger.log_info(f"[Pipeline] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        
        stage_start = datetime.now()
        preprocessing_result = await self.preprocessing.process(current_text, config)
        stage_time = (datetime.now() - stage_start).total_seconds()
        
        stages.append(PipelineStageResult(
            stage_name='preprocessing',
            input_text=current_text[:100],
            output_text=preprocessing_result.cleaned_text[:100],
            processing_time=stage_time,
            success=True,
            details={
                'entities_found': len(preprocessing_result.entities),
                'sentiment': preprocessing_result.sentiment.overall,
                'keywords': len(preprocessing_result.keywords),
                'language': preprocessing_result.language
            }
        ))
        
        error_logger.log_info(f"[Pipeline] Stage 1 complete | Entities: {len(preprocessing_result.entities)} | Sentiment: {preprocessing_result.sentiment.overall}")
        
        stage_start = datetime.now()
        rule_result = await self.rule_engine.process(
            current_text,
            task_id,
            preprocessing_result
        )
        stage_time = (datetime.now() - stage_start).total_seconds()
        
        current_text = rule_result.processed_text
        
        stages.append(PipelineStageResult(
            stage_name='rule_engine',
            input_text=text[:100],
            output_text=current_text[:100],
            processing_time=stage_time,
            success=rule_result.success,
            details={
                'rules_applied': len(rule_result.rules_applied),
                'replacements': rule_result.total_replacements,
                'context_mods': len(rule_result.context_modifications)
            },
            error='; '.join(rule_result.errors) if rule_result.errors else None
        ))
        
        warnings.extend(rule_result.errors)
        
        error_logger.log_info(f"[Pipeline] Stage 2 complete | Rules: {len(rule_result.rules_applied)} | Replacements: {rule_result.total_replacements}")
        
        # âœ… Log detailed rules applied
        if rule_result.rules_applied:
            error_logger.log_info(f"[Pipeline] ğŸ“‹ DETAILED RULES APPLIED:")
            for i, rule_app in enumerate(rule_result.rules_applied, 1):
                error_logger.log_info(f"[Pipeline]   Rule {i}: {rule_app.rule_name} (Type: {rule_app.rule_type})")
                if rule_app.changes_made:
                    error_logger.log_info(f"[Pipeline]     Changes: {len(rule_app.changes_made)} modifications")
        
        # âœ… Log text after rules processing
        error_logger.log_info(f"[Pipeline] ğŸ“ TEXT AFTER RULES: {current_text[:300]}...")
        
        # Skip pre-truncation summarization rule - let AI do the actual summarization
        # The AI will handle both summarization and rule application via custom_rules in prompt
        
        stage_start = datetime.now()
        ai_output, extracted_fields = await self._process_with_ai(
            current_text,
            task_id,
            provider,
            model,
            system_prompt,
            custom_rules,
            preprocessing_result,
            rule_result,
            config,
            video_source_info=video_source_info,
            fields_to_extract=fields_to_extract,
            serial_number=serial_number
        )
        stage_time = (datetime.now() - stage_start).total_seconds()
        
        if ai_output and ai_output.strip():
            stages.append(PipelineStageResult(
                stage_name='ai_summarization',
                input_text=current_text[:100],
                output_text=ai_output[:100],
                processing_time=stage_time,
                success=True,
                details={
                    'provider': provider,
                    'model': model,
                    'input_length': len(current_text),
                    'output_length': len(ai_output)
                }
            ))
            error_logger.log_info(f"[Pipeline] âœ… Stage 3 AI SUCCESS | Input: {len(current_text)} â†’ Output: {len(ai_output)} chars")
            error_logger.log_info(f"[Pipeline] ğŸ“¤ AI OUTPUT: {ai_output[:300]}...")
            current_text = ai_output
        else:
            stages.append(PipelineStageResult(
                stage_name='ai_summarization',
                input_text=current_text[:100],
                output_text=current_text[:100],
                processing_time=stage_time,
                success=False,
                error='AI returned empty response'
            ))
            error_logger.log_warning(f"[Pipeline] âš ï¸ Stage 3 AI FAILED | Empty response from {provider}/{model}")
            error_logger.log_warning(f"[Pipeline] âš ï¸ Using original text (no summarization applied)")
            warnings.append('AI summarization failed, using rule-processed text')
        
        error_logger.log_info(f"[Pipeline] Stage 3 complete | Output length: {len(current_text)}")
        
        stage_start = datetime.now()
        postprocess_result = await self.postprocessing.process(
            current_text,
            text,
            rule_result,
            config
        )
        stage_time = (datetime.now() - stage_start).total_seconds()
        
        final_text = postprocess_result.final_output
        
        stages.append(PipelineStageResult(
            stage_name='postprocessing',
            input_text=current_text[:100],
            output_text=final_text[:100],
            processing_time=stage_time,
            success=postprocess_result.success,
            details={
                'validations': len(postprocess_result.validations),
                'rules_verified': sum(1 for v in postprocess_result.rules_verified.values() if v),
                'quality_score': postprocess_result.quality_score
            }
        ))
        
        warnings.extend(postprocess_result.warnings)
        errors.extend(postprocess_result.errors)
        
        error_logger.log_info(f"[Pipeline] Stage 4 complete | Quality: {postprocess_result.quality_score:.2f}")
        
        total_time = (datetime.now() - start_time).total_seconds()
        
        result = PipelineResult(
            original_text=text,
            final_text=final_text,
            stages=stages,
            preprocessing=preprocessing_result,
            rule_engine=rule_result,
            postprocessing=postprocess_result,
            total_time=total_time,
            quality_score=postprocess_result.quality_score,
            success=all(s.success for s in stages),
            rules_applied_count=len(rule_result.rules_applied),
            entities_replaced=rule_result.entities_replaced,
            warnings=warnings,
            errors=errors,
            extracted_fields=extracted_fields or {}
        )
        
        error_logger.log_info(
            f"[Pipeline] âœ… COMPLETE | Total time: {total_time:.2f}s | "
            f"Quality: {postprocess_result.quality_score:.2f} | "
            f"Original: {len(text)} â†’ Final: {len(final_text)} chars"
        )
        
        reduction_pct = 100 - (len(final_text) * 100 // len(text)) if len(text) > 0 else 0
        error_logger.log_info(
            f"[Pipeline] ğŸ“Š SUMMARY: {len(text)} â†’ {len(final_text)} chars ({reduction_pct}% reduction) | "
            f"Rules: {len(rule_result.rules_applied)} | Entities: {len(rule_result.entities_replaced)}"
        )
        
        return result
    
    async def _process_with_ai(
        self,
        text: str,
        task_id: int,
        provider: str,
        model: str,
        system_prompt: Optional[str],
        custom_rules: Optional[List[Dict]],
        preprocessing_result: PreprocessingResult,
        rule_result: RuleEngineResult,
        config: Dict[str, Any],
        video_source_info: Optional[Dict[str, str]] = None,
        fields_to_extract: Optional[List[Dict[str, Any]]] = None,
        serial_number: Optional[int] = None
    ) -> Tuple[str, Dict[str, Any]]:
        """
        Stage 3: Process text with AI
        Builds enhanced prompt with context from preprocessing and rules
        Includes training examples for few-shot learning
        """
        if not self.ai_manager:
            from services.ai_providers import ai_manager
            self.ai_manager = ai_manager
        
        # âœ… Log input text for AI processing
        error_logger.log_info(f"[Pipeline] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        error_logger.log_info(f"[Pipeline] ğŸ¤– STAGE 3: AI SUMMARIZATION")
        error_logger.log_info(f"[Pipeline] ğŸ“¥ AI INPUT TEXT ({len(text)} chars):")
        error_logger.log_info(f"[Pipeline] {text[:500]}...")
        error_logger.log_info(f"[Pipeline] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        
        training_examples = await self._get_training_examples(task_id)
        error_logger.log_info(f"[Pipeline] ğŸ“š Training Examples | Loaded: {len(training_examples)} for task {task_id}")
        if training_examples:
            for i, ex in enumerate(training_examples[:3], 1):
                error_logger.log_info(f"[Pipeline]   ğŸ“ Example {i}: type={ex.get('example_type')} | input={ex.get('input_text', '')[:50]}... | output={ex.get('expected_output', '')[:50]}...")
        
        # No token limit - let AI produce complete summary without cutoff
        max_tokens = 128000  # No limit - allows full text generation without any restrictions
        if custom_rules:
            error_logger.log_info(f"[Pipeline] âš ï¸ SUMMARIZATION RULES ({len(custom_rules)} rules):")
            for i, rule in enumerate(custom_rules, 1):
                rule_name = rule.get('name', 'Unknown')
                rule_type = rule.get('type', 'unknown')
                rule_prompt = rule.get('prompt', '')
                rule_config = rule.get('config', {})
                error_logger.log_info(f"[Pipeline]   ğŸ“Œ Rule {i}: name={rule_name} | type={rule_type}")
                error_logger.log_info(f"[Pipeline]      prompt={rule_prompt[:100]}...")
                error_logger.log_info(f"[Pipeline]      config={str(rule_config)[:100]}...")
        else:
            error_logger.log_info(f"[Pipeline] âš ï¸ SUMMARIZATION RULES: None provided")
        
        # Initialize extracted fields with serial number
        extracted_fields = {}
        if serial_number is not None:
            serial_val = f"#{serial_number}"
            extracted_fields["Ø±Ù‚Ù…_Ø§Ù„Ù‚ÙŠØ¯"] = serial_val
            extracted_fields["Ø±Ù‚Ù…_Ø§Ù„Ù‚ÙŠØ¯_"] = serial_val
            extracted_fields["serial_number"] = serial_number
            extracted_fields["record_number"] = serial_number
            error_logger.log_info(f"[Pipeline] ğŸ“Œ Injected serial number: {serial_val}")
        
        # Build enhanced prompt including field extraction if needed
        error_logger.log_info(f"[Pipeline] ğŸ“ BUILDING ENHANCED PROMPT...")
        enhanced_prompt = self._build_enhanced_prompt(
            text,
            system_prompt,
            custom_rules,
            preprocessing_result,
            rule_result,
            config,
            training_examples,
            video_source_info=video_source_info,
            fields_to_extract=fields_to_extract
        )
        
        error_logger.log_info(f"[Pipeline] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        error_logger.log_info(f"[Pipeline] ğŸ“‹ AI PROMPT ({len(enhanced_prompt)} chars):")
        error_logger.log_info(f"[Pipeline] {enhanced_prompt[:800]}...")
        error_logger.log_info(f"[Pipeline] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        
        temperature = float(config.get('temperature', '0.7'))
        quality = config.get('quality_level', 'balanced')
        
        # No token restrictions - allow full text completion
        error_logger.log_info(f"[Pipeline] ğŸ”“ Token limit: UNLIMITED (max_tokens={max_tokens} - Ø¨Ø¯ÙˆÙ† Ø­Ø¯ÙˆØ¯)")
        
        try:
            error_logger.log_info(f"[Pipeline] ğŸ”„ CALLING AI")
            error_logger.log_info(f"[Pipeline]    Provider: {provider}")
            error_logger.log_info(f"[Pipeline]    Model: {model}")
            error_logger.log_info(f"[Pipeline]    Max Tokens: {max_tokens}")
            error_logger.log_info(f"[Pipeline]    Temperature: {temperature}")
            
            result = await self.ai_manager.generate(
                provider=provider,
                model=model,
                prompt=enhanced_prompt,
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            if not result:
                error_logger.log_warning(f"[Pipeline] âš ï¸ AI returned EMPTY result (provider={provider}, model={model})")
                return ("", extracted_fields)
            
            error_logger.log_info(f"[Pipeline] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
            error_logger.log_info(f"[Pipeline] âœ… AI RETURNED {len(result)} chars")
            error_logger.log_info(f"[Pipeline] ğŸ“¤ AI OUTPUT (BEFORE RULES):")
            error_logger.log_info(f"[Pipeline] {result[:500]}...")
            error_logger.log_info(f"[Pipeline] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
            
            # Parse result - check if it contains JSON with extracted fields
            if fields_to_extract:
                summary_text, ai_extracted = self._parse_combined_response(result, fields_to_extract)
                extracted_fields.update(ai_extracted)
                error_logger.log_info(f"[Pipeline] ğŸ“¦ Extracted {len(ai_extracted)} fields in single AI call")
                
                # âœ… FIXED: Apply summarization rules to extracted summary
                summary_text = await self._apply_post_summarization_rules(summary_text, task_id)
                error_logger.log_info(f"[Pipeline] âœ… Post-summarization rules applied | Final: {len(summary_text)} chars")
                
                return (summary_text, extracted_fields)
            
            # âœ… FIXED: Apply summarization rules to direct AI output
            final_result = await self._apply_post_summarization_rules(result, task_id)
            error_logger.log_info(f"[Pipeline] âœ… Post-summarization rules applied | Final: {len(final_result)} chars")
            error_logger.log_info(f"[Pipeline] ğŸ“¤ FINAL OUTPUT (AFTER RULES): {final_result[:500]}...")
            
            return (final_result or "", extracted_fields)
        except Exception as e:
            error_logger.log_warning(f"[Pipeline] âŒ AI generation EXCEPTION: {str(e)} (provider={provider}, model={model})")
            import traceback
            error_logger.log_warning(f"[Pipeline] Traceback: {traceback.format_exc()}")
            return ("", extracted_fields)
    
    async def _apply_post_summarization_rules(self, text: str, task_id: int) -> str:
        """
        âœ… FIXED: Apply summarization rules to AI output
        Supports maxLength, style, keyPointsCount from database rules
        Also cleans up unwanted labels like 'Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©:' and 'Ø§Ù„Ù…ØµØ¯Ø±:'
        """
        try:
            if not text:
                return text
            
            # Clean up common unwanted labels from the summary text
            unwanted_labels = [
                "Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©:", "Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø© :", "**Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©:**",
                "Ø§Ù„Ù…ØµØ¯Ø±:", "Ø§Ù„Ù…ØµØ¯Ø± :", "**Ø§Ù„Ù…ØµØ¯Ø±:**",
                "Ø§Ù„ØªØµÙ†ÙŠÙ:", "Ø§Ù„ØªØµÙ†ÙŠÙ :", "**Ø§Ù„ØªØµÙ†ÙŠÙ:**",
                "Ø±Ù‚Ù… Ø§Ù„Ù‚ÙŠØ¯:", "Ø±Ù‚Ù… Ø§Ù„Ù‚ÙŠØ¯ :", "**Ø±Ù‚Ù… Ø§Ù„Ù‚ÙŠØ¯:**"
            ]
            
            cleaned_text = text
            for label in unwanted_labels:
                # Remove label and any following whitespace/newlines until the next word
                import re
                cleaned_text = re.sub(rf"{re.escape(label)}\s*", "", cleaned_text)
            
            text = cleaned_text.strip()
            
            from utils.database import db
            ai_rules = await db.get_task_rules(task_id)
            if not ai_rules:
                return text
            
            # Filter for active summarize rules
            summarize_rules = []
            for r in ai_rules:
                rule_dict = dict(r) if hasattr(r, '__iter__') and not isinstance(r, dict) else r
                if isinstance(rule_dict, dict) and rule_dict.get('type') == 'summarize' and rule_dict.get('is_active'):
                    summarize_rules.append(rule_dict)
            
            if not summarize_rules:
                return text
            
            processed_text = text
            for rule in sorted(summarize_rules, key=lambda r: r.get('priority', 0), reverse=True):
                rule_config = rule.get('config', {})
                if isinstance(rule_config, str):
                    try:
                        import json
                        rule_config = json.loads(rule_config)
                    except:
                        rule_config = {}
                
                # Extract max_length and apply truncation
                max_length = rule_config.get('maxLength') or rule_config.get('max_length')
                if max_length and isinstance(max_length, (int, float)) and max_length > 0:
                    if len(processed_text) > max_length:
                        processed_text = processed_text[:int(max_length)].rsplit(' ', 1)[0]
                        if not processed_text.endswith(('...', 'ØŒ', '.', 'ØŸ')):
                            processed_text += '...'
                        error_logger.log_info(f"[Pipeline] âœ… Applied max_length({max_length}): {len(text)} â†’ {len(processed_text)} chars")
            
            return processed_text
        except Exception as e:
            error_logger.log_warning(f"[Pipeline] âš ï¸ Error in post-summarization rules: {str(e)}")
            return text
    
    def _build_enhanced_prompt(
        self,
        text: str,
        system_prompt: Optional[str],
        custom_rules: Optional[List[Dict]],
        preprocessing_result: PreprocessingResult,
        rule_result: RuleEngineResult,
        config: Dict[str, Any],
        training_examples: Optional[List[Dict]] = None,
        video_source_info: Optional[Dict[str, str]] = None,
        fields_to_extract: Optional[List[Dict[str, Any]]] = None
    ) -> str:
        """
        Build enhanced prompt with all context, rules, and training examples
        Includes video metadata (title, description, uploader) for better summarization
        """
        prompt_parts = []
        
        base_prompt = system_prompt or """Ø£Ù†Øª Ù…Ø­Ø±Ø± Ø¥Ø®Ø¨Ø§Ø±ÙŠ Ù…Ø­ØªØ±Ù ÙˆÙ…Ø­Ù„Ù„ Ù„ØºÙˆÙŠ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØªØ­Ø±ÙŠØ± ÙˆØªÙ„Ø®ÙŠØµ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙˆØ§Ù„ØªÙ‚Ø§Ø±ÙŠØ±.

ğŸ¯ Ù…Ù‡Ø§Ù…Ùƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨):
1. ØªØ·Ø¨ÙŠÙ‚ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø°ÙƒÙŠ (Ø¥Ø°Ø§ ÙˆÙØ¬Ø¯Øª) - Ù‡Ø°Ù‡ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„Ù‚ØµÙˆÙ‰
2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø¨Ø¯Ù‚Ø© Ù…Ù† ÙƒØ§Ù…Ù„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±ÙÙ‚ (Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Ø§Ù„ÙƒØ§Ø¨Ø´Ù† ÙˆØ§Ù„Ù†Øµ Ø§Ù„Ù…ÙØ±Øº)
3. ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ Ø¨Ø´ÙƒÙ„ Ù…ÙˆØ¬Ø² ÙˆØ¯Ù‚ÙŠÙ‚ (Ù…Ù„Ø§Ø­Ø¸Ø© Ù‡Ø§Ù…Ø© ÙˆØ­Ø§Ø³Ù…Ø©: Ù„Ø§ ØªÙƒØ±Ø± Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© Ù…Ø«Ù„ Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø© ÙˆØ§Ù„Ù…ØµØ¯Ø± Ø¯Ø§Ø®Ù„ Ù†Øµ Ø§Ù„ØªÙ„Ø®ÙŠØµ Ù†ÙØ³Ù‡ØŒ ÙˆÙ„Ø§ ØªØ¶Ø¹ Ø£ÙŠ Ø¹Ù†Ø§ÙˆÙŠÙ† Ø¬Ø§Ù†Ø¨ÙŠØ© Ù…Ø«Ù„ "Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©:" Ø£Ùˆ "Ø§Ù„Ù…ØµØ¯Ø±:" ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØŒ Ø£Ø±ÙŠØ¯ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù„Ø®Øµ ÙÙ‚Ø·)
4. Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø­ÙŠØ§Ø¯ ÙˆØ§Ù„Ù…ÙˆØ¶ÙˆØ¹ÙŠØ©
5. Ø§Ø³ØªØ®Ø¯Ø§Ù… ØµÙŠØ§ØºØ© Ø±Ø³Ù…ÙŠØ© ÙˆÙ…Ù‡Ù†ÙŠØ©

ğŸ” ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø®Ø§ØµØ© Ø¨Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬:
â€¢ Ø§Ø³ØªØ®Ø±Ø¬ "Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©" Ù…Ù† Ø§Ù„Ù†Øµ Ø¥Ø°Ø§ Ø°Ø±Øª Ø£ÙŠ Ù…Ø¯ÙŠÙ†Ø© Ø£Ùˆ Ù…Ù†Ø·Ù‚Ø© ÙŠÙ…Ù†ÙŠØ© (Ù…Ø«Ù„: Ø¹Ø¯Ù†ØŒ ØµÙ†Ø¹Ø§Ø¡ØŒ Ù…Ø£Ø±Ø¨ØŒ ØªÙ„ Ø£Ø¨ÙŠØ¨ØŒ Ù…Ø£Ø±Ø¨ØŒ ØªØ¹Ø²ØŒ Ø¥Ù„Ø®). Ø¥Ø°Ø§ Ù„Ù… ØªØ°ÙƒØ± Ù…Ø­Ø§ÙØ¸Ø©ØŒ Ø§ØªØ±Ùƒ Ø§Ù„Ø­Ù‚Ù„ ÙØ§Ø±ØºØ§Ù‹.
â€¢ Ø§Ø³ØªØ®Ø±Ø¬ "Ø§Ù„Ù…ØµØ¯Ø±" Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ù‡Ø© Ø§Ù„Ù†Ø§Ù‚Ù„Ø© Ù„Ù„Ø®Ø¨Ø± Ø£Ùˆ Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ù…ØªØ­Ø¯Ø«ÙŠÙ† Ø£Ùˆ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø°ÙƒÙˆØ± ÙÙŠ Ø§Ù„Ù†Øµ Ø£Ùˆ Ø§Ù„ÙƒØ§Ø¨Ø´Ù†.
â€¢ Ø§Ø¨Ø­Ø« ÙÙŠ ÙƒØ§Ù…Ù„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ù…ÙˆØ¬ (Ø§Ù„ÙƒØ§Ø¨Ø´Ù† + Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ) Ø¹Ù† Ù‡Ø°Ù‡ Ø§Ù„ØªÙØ§ØµÙŠÙ„."""
        
        prompt_parts.append(base_prompt)
        
        # Add video metadata if available (title, description, uploader)
        if video_source_info:
            metadata_text = "\n\nğŸ“¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ/Ø§Ù„Ù…Ø­ØªÙˆÙ‰:"
            
            if video_source_info.get('title'):
                metadata_text += f"\nâ€¢ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {video_source_info.get('title')}"
            
            if video_source_info.get('description'):
                desc = video_source_info.get('description', '')
                # Truncate if too long
                if len(desc) > 500:
                    desc = desc[:500] + "..."
                metadata_text += f"\nâ€¢ Ø§Ù„ÙˆØµÙ: {desc}"
            
            if video_source_info.get('uploader'):
                metadata_text += f"\nâ€¢ Ø§Ù„Ù…ØµØ¯Ø±/Ø§Ù„Ù…Ø­Ù…Ù„: {video_source_info.get('uploader')}"
            
            if video_source_info.get('platform'):
                metadata_text += f"\nâ€¢ Ø§Ù„Ù…Ù†ØµØ©: {video_source_info.get('platform')}"
            
            if video_source_info.get('duration'):
                duration = video_source_info.get('duration', 0)
                if isinstance(duration, (int, float)) and duration > 0:
                    metadata_text += f"\nâ€¢ Ø§Ù„Ù…Ø¯Ø©: {int(duration)} Ø«Ø§Ù†ÙŠØ©"
            
            metadata_text += "\n\nâš ï¸ Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù„ØªØ­Ø³ÙŠÙ† ÙÙ‡Ù…Ùƒ Ù„Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ø¯Ù‚ÙŠÙ‚"
            prompt_parts.append(metadata_text)
        
        # Add summarization options from config
        summ_config = config.get('config', {}) if isinstance(config.get('config'), dict) else {}
        max_length = summ_config.get('maxLength', 300)
        style = summ_config.get('style', 'balanced')
        key_points = summ_config.get('keyPointsCount', 3)
        
        if max_length or style:
            options_text = f"\n\nØ®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªÙ„Ø®ÙŠØµ:"
            options_text += f"\n- Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ù‚ØµÙ‰: {max_length} Ø­Ø±Ù"
            options_text += f"\n- Ø§Ù„Ø£Ø³Ù„ÙˆØ¨: {style}"
            if key_points:
                options_text += f"\n- Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {key_points}"
            prompt_parts.append(options_text)
        
        if custom_rules:
            rules_text = "\n\nâš ï¸ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠØ© (ÙŠØ¬Ø¨ ØªØ·Ø¨ÙŠÙ‚Ù‡Ø§ Ø¨Ø¯Ù‚Ø©):"
            for i, rule in enumerate(custom_rules, 1):
                rule_prompt = rule.get('prompt', '')
                rule_type = rule.get('type', '')
                
                # For summarization rules, build enhanced prompt from config
                if rule_type == 'summarize' and not rule_prompt:
                    rule_config = rule.get('config', {})
                    
                    # Parse config if it's a JSON string
                    if isinstance(rule_config, str):
                        try:
                            import json
                            rule_config = json.loads(rule_config)
                        except:
                            rule_config = {}
                    
                    max_length = rule_config.get('maxLength', 300) if isinstance(rule_config, dict) else 300
                    style = rule_config.get('style', 'balanced') if isinstance(rule_config, dict) else 'balanced'
                    rule_prompt = f"Ù‚Ù… Ø¨ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ Ù„ÙŠÙƒÙˆÙ† {style} Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ (Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰: {max_length} Ø­Ø±Ù)"
                
                if rule_prompt:
                    rules_text += f"\n{i}. {rule_prompt}"
                    
                    # Add special handling for news format requirements
                    if 'Ø®Ø¨Ø±' in rule_prompt or 'Ø£Ø³Ù„ÙˆØ¨ Ø¥Ø®Ø¨Ø§Ø±ÙŠ' in rule_prompt or 'news' in rule_prompt.lower():
                        rules_text += "\n   ğŸ“° ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø®Ø¨Ø± Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠ:"
                        rules_text += "\n   â€¢ Ø§ÙƒØªØ¨ Ø§Ù„Ø®Ø¨Ø± ÙÙŠ Ø´ÙƒÙ„ ÙÙ‚Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ù…ØªØ³Ù„Ø³Ù„Ø©"
                        rules_text += "\n   â€¢ Ø§Ø¨Ø¯Ø£ Ø¨Ø£Ù‡Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª (Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©)"
                        rules_text += "\n   â€¢ Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ù‚Ø§Ø· (â€¢) Ø£Ùˆ Ø§Ù„ØªØ±Ù‚ÙŠÙ…"
                        rules_text += "\n   â€¢ Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ ÙˆØ§Ù„Ø³Ù„Ø§Ø³Ø©"
                        rules_text += "\n   â€¢ Ø§Ø¬Ø¹Ù„ Ø§Ù„ØµÙŠØ§ØºØ© Ø§Ø­ØªØ±Ø§ÙÙŠØ© ÙˆØ¥Ø®Ø¨Ø§Ø±ÙŠØ©"
                    
            
            if len(rules_text) > len("\n\nâš ï¸ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠØ© (ÙŠØ¬Ø¨ ØªØ·Ø¨ÙŠÙ‚Ù‡Ø§ Ø¨Ø¯Ù‚Ø©):"):
                prompt_parts.append(rules_text)
        
        if rule_result.entities_replaced:
            entity_instructions = """

ğŸ”– Ø§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ØªØ·Ø¨ÙŠÙ‚Ù‡Ø§ Ù…Ø³Ø¨Ù‚Ø§Ù‹ (ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… Ø¥Ø¹Ø§Ø¯ØªÙ‡Ø§ Ù„Ù„Ø£ØµÙ„):
"""
            for entity_type, replacements in rule_result.entities_replaced.items():
                entity_instructions += f"\nğŸ“Œ Ù†ÙˆØ¹ Ø§Ù„ÙƒÙŠØ§Ù†: {entity_type}"
                for original, replacement in replacements:
                    entity_instructions += f"\n   â€¢ '{original}' â† ØªÙ… Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡Ø§ Ø¨Ù€ â† '{replacement}'"
                    entity_instructions += f"\n     âš¡ ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø£ÙŠ ØµÙŠØºØ© Ø£Ø®Ø±Ù‰ Ù„Ù€ '{original}' Ø¨Ù€ '{replacement}'"
            
            entity_instructions += """

âš ï¸ Ù…Ù„Ø§Ø­Ø¸Ø© Ù…Ù‡Ù…Ø©: Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª Ø£ÙŠ Ø°ÙƒØ± Ø¢Ø®Ø± Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡ØŒ Ù‚Ù… Ø¨Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡.
"""
            prompt_parts.append(entity_instructions)
        
        # Add semantic replacement rules for AI to find and replace variations
        if hasattr(rule_result, 'semantic_replacement_rules') and rule_result.semantic_replacement_rules:
            semantic_instructions = """

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸ Ù…Ù‡Ù…Ø© Ø§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø°ÙƒÙŠ (Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„Ù‚ØµÙˆÙ‰ - ÙŠØ¬Ø¨ ØªÙ†ÙÙŠØ°Ù‡Ø§ Ø¨Ø¯Ù‚Ø© Ù…ØªÙ†Ø§Ù‡ÙŠØ©)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù…Ù†Ùƒ:
Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ø³Ø·Ø±Ø§Ù‹ Ø¨Ø³Ø·Ø±ØŒ ÙƒÙ„Ù…Ø© Ø¨ÙƒÙ„Ù…Ø©ØŒ ÙˆØ§Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ Ø°ÙƒØ± Ù„Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ø£Ø¯Ù†Ø§Ù‡.
ÙŠØ¬Ø¨ Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡Ø§ Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù†Øª Ù…ÙƒØªÙˆØ¨Ø© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø®ØªÙ„ÙØ© Ø£Ùˆ ÙÙŠ Ø³ÙŠØ§Ù‚ Ù…Ø®ØªÙ„Ù.

ğŸ” Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:
1ï¸âƒ£ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ: Ø§ÙÙ‡Ù… Ù…Ø¹Ù†Ù‰ ÙƒÙ„ Ø¬Ù…Ù„Ø© ÙˆØ­Ø¯Ø¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ØªØ´ÙŠØ± Ù„Ù„ÙƒÙŠØ§Ù† Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡
2ï¸âƒ£ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ: Ø§Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…ØªØ±Ø§Ø¯ÙØ§Øª ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø°Ø§Øª Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡
3ï¸âƒ£ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ: Ø§Ø¨Ø­Ø« Ø¹Ù† Ø¬Ù…ÙŠØ¹ ØªØµØ±ÙŠÙØ§Øª Ø§Ù„ÙƒÙ„Ù…Ø© (Ø¬Ù…Ø¹ØŒ Ù…ÙØ±Ø¯ØŒ Ù…Ø¤Ù†Ø«ØŒ Ù…Ø°ÙƒØ±ØŒ Ù…Ø¶Ø§Ù)
4ï¸âƒ£ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ù…Ù„Ø§Ø¦ÙŠ: ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø© Ø­ØªÙ‰ Ù…Ø¹ Ø£Ø®Ø·Ø§Ø¡ Ø¥Ù…Ù„Ø§Ø¦ÙŠØ© Ø£Ùˆ Ø§Ø®ØªÙ„Ø§Ù ÙÙŠ Ø§Ù„Ù‡Ù…Ø²Ø§Øª
5ï¸âƒ£ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª: Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ ÙŠØ´ÙŠØ± Ù„Ù„ÙƒÙŠØ§Ù† Ø¨Ø¶Ù…ÙŠØ± Ø£Ùˆ Ù„Ù‚Ø¨ Ø£Ùˆ ÙˆØµÙØŒ Ø§Ø³ØªØ¨Ø¯Ù„Ù‡ Ø£ÙŠØ¶Ø§Ù‹

ğŸ“Œ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†Ù‡Ø§:
â€¢ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø­Ø±ÙÙŠØ©: Ù†ÙØ³ Ø§Ù„ÙƒÙ„Ù…Ø© Ø¨Ø§Ù„Ø¶Ø¨Ø·
â€¢ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„ØµØ±ÙÙŠØ©: Ø§Ù„Ø¬Ù…Ø¹ ÙˆØ§Ù„Ù…ÙØ±Ø¯ (Ù…Ù„ÙŠØ´ÙŠØ§/Ù…Ù„ÙŠØ´ÙŠØ§ØªØŒ Ø­ÙˆØ«ÙŠ/Ø­ÙˆØ«ÙŠÙŠÙ†/Ø­ÙˆØ«ÙŠÙˆÙ†)
â€¢ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ø¹ Ø§Ù„ØªØ¹Ø±ÙŠÙ: Ù…Ø¹ "Ø§Ù„" Ø£Ùˆ Ø¨Ø¯ÙˆÙ†Ù‡Ø§ (Ø§Ù„Ø­ÙˆØ«ÙŠ/Ø­ÙˆØ«ÙŠØŒ Ø§Ù„Ø¬ÙŠØ´/Ø¬ÙŠØ´)
â€¢ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ø¹ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø±: Ø§Ù„ÙƒÙ„Ù…Ø© Ù…Ø¹ Ø¶Ù…Ø§Ø¦Ø± Ù…ØªØµÙ„Ø© (Ø¬ÙŠØ´Ù‡ØŒ Ø¬ÙŠØ´Ù‡Ù…ØŒ Ù…Ù„ÙŠØ´ÙŠØ§ØªÙ‡Ù…)
â€¢ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ©: Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙØ´Ø§Ø± Ù„Ù„ÙƒÙŠØ§Ù† Ø¨ÙˆØµÙ Ø£Ùˆ Ù„Ù‚Ø¨ Ù…Ø¹Ø±ÙˆÙ
â€¢ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø¬Ø²Ø¦ÙŠØ©: Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ÙƒÙ„Ù…Ø© Ø¬Ø²Ø¡Ø§Ù‹ Ù…Ù† Ø¹Ø¨Ø§Ø±Ø© Ø£Ø·ÙˆÙ„
â€¢ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø¥Ù…Ù„Ø§Ø¦ÙŠØ©: Ù†ÙØ³ Ø§Ù„ÙƒÙ„Ù…Ø© Ø¨Ø£Ø®Ø·Ø§Ø¡ Ø¥Ù…Ù„Ø§Ø¦ÙŠØ© Ø´Ø§Ø¦Ø¹Ø©

ğŸ¯ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠØ©:
"""
            for rule in rule_result.semantic_replacement_rules:
                originals = rule.get('originals', [])
                replacements = rule.get('replacements', [])
                if not originals and rule.get('original'):
                    originals = [rule.get('original')]
                if not replacements and rule.get('replacement'):
                    replacements = [rule.get('replacement')]
                
                if originals and replacements:
                    primary_replacement = replacements[0] if replacements[0] else ""
                    originals_str = 'ØŒ '.join([str(o) for o in originals if o])
                    
                    semantic_instructions += f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ ğŸ”„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©: {originals_str}
â”‚ âœ… Ø§Ù„Ø¨Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: {primary_replacement}
â”‚ 
â”‚ ğŸ“ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø®Ø§ØµØ© Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø©:
â”‚ â€¢ Ø§Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ Ø°ÙƒØ± Ù…Ø¨Ø§Ø´Ø± Ø£Ùˆ ØºÙŠØ± Ù…Ø¨Ø§Ø´Ø± Ù„Ù‡Ø°Ù‡ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
â”‚ â€¢ Ø§Ø³ØªØ¨Ø¯Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØµØ±ÙŠÙØ§Øª: (Ù…ÙØ±Ø¯/Ø¬Ù…Ø¹/Ù…Ø°ÙƒØ±/Ù…Ø¤Ù†Ø«/Ù…Ø¹Ø±Ù/Ù†ÙƒØ±Ø©)
â”‚ â€¢ Ø§Ø³ØªØ¨Ø¯Ù„ Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù†Øª Ù…Ø¹ Ø¶Ù…Ø§Ø¦Ø± Ù…ØªØµÙ„Ø© Ø£Ùˆ Ø­Ø±ÙˆÙ Ø¬Ø±
â”‚ â€¢ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠØ´ÙŠØ± Ù„Ù†ÙØ³ Ø§Ù„ÙƒÙŠØ§Ù† Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø®ØªÙ„ÙØ©ØŒ Ø§Ø³ØªØ¨Ø¯Ù„Ù‡
â”‚ â€¢ Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø³Ù„Ø§Ø³Ø© Ø§Ù„Ù†Øµ ÙˆØµØ­Ø© Ø§Ù„Ø¥Ø¹Ø±Ø§Ø¨ Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"""
            
            semantic_instructions += """
âš¡ Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ:
Ù…Ø«Ø§Ù„ 1: Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø§Ø³ØªØ¨Ø¯Ø§Ù„ "Ù…Ù„ÙŠØ´ÙŠØ§ Ø§Ù„Ø­ÙˆØ«ÙŠ" Ø¨Ù€ "Ø¬Ù…Ø§Ø¹Ø© Ø£Ù†ØµØ§Ø± Ø§Ù„Ù„Ù‡"
   - "Ù…Ù„ÙŠØ´ÙŠØ§ Ø§Ù„Ø­ÙˆØ«ÙŠ" â† "Ø¬Ù…Ø§Ø¹Ø© Ø£Ù†ØµØ§Ø± Ø§Ù„Ù„Ù‡" âœ“
   - "Ù…Ù„ÙŠØ´ÙŠØ§Øª Ø§Ù„Ø­ÙˆØ«ÙŠ" â† "Ø¬Ù…Ø§Ø¹Ø© Ø£Ù†ØµØ§Ø± Ø§Ù„Ù„Ù‡" âœ“
   - "Ø§Ù„Ù…ÙŠÙ„ÙŠØ´ÙŠØ§ Ø§Ù„Ø­ÙˆØ«ÙŠØ©" â† "Ø¬Ù…Ø§Ø¹Ø© Ø£Ù†ØµØ§Ø± Ø§Ù„Ù„Ù‡" âœ“
   - "Ù…Ù„ÙŠØ´ÙŠØ§ØªÙ‡Ù…" â† "Ø§Ù„Ø¬Ù…Ø§Ø¹Ø©" âœ“
   - "Ø§Ù„Ù…Ù„ÙŠØ´ÙŠØ§" (Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠØ´ÙŠØ± Ù„Ù„Ø­ÙˆØ«ÙŠ) â† "Ø¬Ù…Ø§Ø¹Ø© Ø£Ù†ØµØ§Ø± Ø§Ù„Ù„Ù‡" âœ“

Ù…Ø«Ø§Ù„ 2: Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø§Ø³ØªØ¨Ø¯Ø§Ù„ "Ø§Ù„Ø¥Ø±Ù‡Ø§Ø¨ÙŠÙŠÙ†" Ø¨Ù€ "Ø§Ù„Ù…Ø³Ù„Ø­ÙŠÙ†"
   - "Ø§Ù„Ø¥Ø±Ù‡Ø§Ø¨ÙŠÙŠÙ†" â† "Ø§Ù„Ù…Ø³Ù„Ø­ÙŠÙ†" âœ“
   - "Ø¥Ø±Ù‡Ø§Ø¨ÙŠ" â† "Ù…Ø³Ù„Ø­" âœ“
   - "Ø§Ù„Ø¥Ø±Ù‡Ø§Ø¨ÙŠÙˆÙ†" â† "Ø§Ù„Ù…Ø³Ù„Ø­ÙˆÙ†" âœ“
   - "Ø¥Ø±Ù‡Ø§Ø¨ÙŠØ©" â† "Ù…Ø³Ù„Ø­Ø©" âœ“
   - "Ø§Ù„Ø¬Ù…Ø§Ø¹Ø© Ø§Ù„Ø¥Ø±Ù‡Ø§Ø¨ÙŠØ©" â† "Ø§Ù„Ø¬Ù…Ø§Ø¹Ø© Ø§Ù„Ù…Ø³Ù„Ø­Ø©" âœ“

ğŸš« ØªØ­Ø°ÙŠØ±Ø§Øª Ù…Ù‡Ù…Ø©:
â€¢ Ù„Ø§ ØªØ³ØªØ¨Ø¯Ù„ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ÙƒÙ„Ù…Ø© ÙÙŠ Ø³ÙŠØ§Ù‚ Ù…Ø®ØªÙ„Ù ØªÙ…Ø§Ù…Ø§Ù‹ Ù„Ø§ Ø¹Ù„Ø§Ù‚Ø© Ù„Ù‡ Ø¨Ø§Ù„ÙƒÙŠØ§Ù† Ø§Ù„Ù…Ù‚ØµÙˆØ¯
â€¢ Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ø¬Ù…Ù„Ø©
â€¢ ØªØ£ÙƒØ¯ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¥Ø¹Ø±Ø§Ø¨ ÙˆØ§Ù„ØªØ°ÙƒÙŠØ± ÙˆØ§Ù„ØªØ£Ù†ÙŠØ« Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„
â€¢ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ÙƒÙ„Ù…Ø© Ø¶Ù…Ù† Ø§Ù‚ØªØ¨Ø§Ø³ Ø­Ø±ÙÙŠØŒ Ø§Ø³ØªØ¨Ø¯Ù„Ù‡Ø§ Ø£ÙŠØ¶Ø§Ù‹

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
            prompt_parts.append(semantic_instructions)
            error_logger.log_info(f"[Pipeline] Added {len(rule_result.semantic_replacement_rules)} enhanced semantic replacement rules to prompt")
        
        # Add context rules instructions for AI processing
        if hasattr(rule_result, 'ai_instructions') and rule_result.ai_instructions:
            context_instructions = """

ğŸ“ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„ØªØ­Ø±ÙŠØ± Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ (Ø¥Ù„Ø²Ø§Ù…ÙŠØ©):
Ù‡Ø°Ù‡ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª ØªØ­Ø¯Ø¯ ÙƒÙŠÙÙŠØ© Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø³ÙŠØ§Ù‚Ø§Øª Ù…Ø¹ÙŠÙ†Ø© ÙÙŠ Ø§Ù„Ù†Øµ.
"""
            for i, inst in enumerate(rule_result.ai_instructions, 1):
                instructions_text = inst.get('instructions', '')
                rule_type = inst.get('rule_type', '')
                target_sentiment = inst.get('target_sentiment', 'neutral')
                if instructions_text:
                    sentiment_label = {
                        'positive': 'ğŸŸ¢ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ',
                        'negative': 'ğŸ”´ Ø³Ù„Ø¨ÙŠ', 
                        'neutral': 'âšª Ù…Ø­Ø§ÙŠØ¯'
                    }.get(target_sentiment, 'âšª Ù…Ø­Ø§ÙŠØ¯')
                    
                    context_instructions += f"""
â”Œâ”€â”€ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø© {i} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ ğŸ“‹ {instructions_text}
â”‚ ğŸ¯ Ø§Ù„Ù†Ø¨Ø±Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©: {sentiment_label}
â”‚ ğŸ’¡ Ø·Ø¨Ù‚ Ù‡Ø°Ù‡ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø© Ø¹Ù„Ù‰ ÙƒÙ„ Ø¬Ø²Ø¡ Ù…Ù†Ø§Ø³Ø¨ ÙÙŠ Ø§Ù„Ù†Øµ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"""
            prompt_parts.append(context_instructions)
            error_logger.log_info(f"[Pipeline] Added {len(rule_result.ai_instructions)} context instructions to prompt")
        
        if preprocessing_result.sentiment.has_offensive:
            offensive_instructions = """

âš ï¸ ØªÙ†Ø¨ÙŠÙ‡: ØªÙ… Ø±ØµØ¯ Ù„ØºØ© ØºÙŠØ± Ù…Ù†Ø§Ø³Ø¨Ø© ÙÙŠ Ø§Ù„Ù†Øµ

ğŸ“Œ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª:
â€¢ Ø­ÙŠÙ‘Ø¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ù„ÙØ§Ø¸ Ø§Ù„Ù…Ø³ÙŠØ¦Ø© Ø£Ùˆ Ø§Ù„Ø¹Ù†ÙŠÙØ©
â€¢ Ø§Ø³ØªØ¨Ø¯Ù„ Ø§Ù„Ø´ØªØ§Ø¦Ù… ÙˆØ§Ù„Ø¥Ù‡Ø§Ù†Ø§Øª Ø¨ÙˆØµÙ Ù…Ø­Ø§ÙŠØ¯
â€¢ Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø¯ÙˆÙ† Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¬Ø§Ø±Ø­Ø©
â€¢ Ø§Ø¬Ø¹Ù„ Ø§Ù„ØµÙŠØ§ØºØ© Ù…Ù‡Ù†ÙŠØ© ÙˆÙ…Ø­Ø§ÙŠØ¯Ø©

ğŸ”„ Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØ­ÙŠÙŠØ¯:
â€¢ "Ø¥Ø±Ù‡Ø§Ø¨ÙŠ/Ù…Ø¬Ø±Ù…" â†’ "Ù…Ø³Ù„Ø­/Ù…ØªÙ‡Ù…"
â€¢ "Ø¹ØµØ§Ø¨Ø©" â†’ "Ù…Ø¬Ù…ÙˆØ¹Ø©"  
â€¢ "ÙƒÙ„Ø¨/Ø­Ù…Ø§Ø±" â†’ "Ø´Ø®Øµ"
â€¢ Ø§Ù„Ø´ØªØ§Ø¦Ù… â†’ Ø­Ø°ÙÙ‡Ø§ Ø£Ùˆ Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡Ø§ Ø¨ÙˆØµÙ Ù…Ø­Ø§ÙŠØ¯
"""
            prompt_parts.append(offensive_instructions)
        
        if training_examples:
            examples_section = "\n\nğŸ“š Ø£Ù…Ø«Ù„Ø© ØªØ¯Ø±ÙŠØ¨ÙŠØ© (ØªØ¹Ù„Ù… Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø£Ù…Ø«Ù„Ø© ÙˆØ§ØªØ¨Ø¹ Ù†ÙØ³ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨):"
            for i, example in enumerate(training_examples[:5], 1):
                input_text = example.get('input_text', '')[:200]
                expected_output = example.get('expected_output', '')[:200]
                explanation = example.get('explanation', '')
                example_type = example.get('example_type', 'general')
                
                examples_section += f"\n\n--- Ù…Ø«Ø§Ù„ {i} ({example_type}) ---"
                examples_section += f"\nâ—€ï¸ Ø§Ù„Ù…Ø¯Ø®Ù„: {input_text}"
                examples_section += f"\nâ–¶ï¸ Ø§Ù„Ù…Ø®Ø±Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: {expected_output}"
                if explanation:
                    examples_section += f"\nğŸ’¡ Ø§Ù„Ø³Ø¨Ø¨: {explanation}"
            
            examples_section += "\n\n--- Ø§Ù†ØªÙ‡Øª Ø§Ù„Ø£Ù…Ø«Ù„Ø© ---"
            examples_section += "\nØ§ØªØ¨Ø¹ Ù†ÙØ³ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ ÙˆØ§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ."
            prompt_parts.append(examples_section)
            error_logger.log_info(f"[Pipeline] Added {min(len(training_examples), 5)} training examples to prompt")
        
        preserve_format = config.get('preserve_formatting', True)
        output_format = config.get('output_format', 'markdown')
        
        format_instructions = """

ğŸ“‹ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:
"""
        if output_format == 'markdown':
            format_instructions += "â€¢ Ø§Ø³ØªØ®Ø¯Ù… ØªÙ†Ø³ÙŠÙ‚ Markdown Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø© Ù„Ù„ØªÙ†Ø¸ÙŠÙ…\n"
            format_instructions += "â€¢ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØ§Ù„Ù†Ù‚Ø§Ø· Ù„ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ù…Ø­ØªÙˆÙ‰\n"
        elif output_format == 'plain':
            format_instructions += "â€¢ Ø£Ø®Ø±Ø¬ Ù†ØµØ§Ù‹ Ø¹Ø§Ø¯ÙŠØ§Ù‹ Ø¨Ø¯ÙˆÙ† ØªÙ†Ø³ÙŠÙ‚\n"
        
        format_instructions += """â€¢ Ù„Ø§ ØªØ¨Ø¯Ø£ Ø¨ÙƒÙ„Ù…Ø© 'Ù…Ù„Ø®Øµ' Ø£Ùˆ 'Summary'
â€¢ Ù„Ø§ ØªØ°ÙƒØ± Ø£Ù†Ùƒ Ù†Ù…ÙˆØ°Ø¬ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
â€¢ Ù„Ø§ ØªØ´Ø±Ø­ Ù…Ø§ Ù‚Ù…Øª Ø¨Ù‡ØŒ ÙÙ‚Ø· Ø£Ø®Ø±Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ

âœ… Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬:
â–¡ Ù‡Ù„ Ø·Ø¨Ù‚Øª Ø¬Ù…ÙŠØ¹ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„ØŸ
â–¡ Ù‡Ù„ Ø¨Ø­Ø«Øª Ø¹Ù† Ø¬Ù…ÙŠØ¹ ØµÙŠØº Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡Ø§ØŸ
â–¡ Ù‡Ù„ Ø­Ø§ÙØ¸Øª Ø¹Ù„Ù‰ Ø³Ù„Ø§Ø³Ø© Ø§Ù„Ù†Øµ Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„ØŸ
â–¡ Ù‡Ù„ ØµØ­Ø© Ø§Ù„Ø¥Ø¹Ø±Ø§Ø¨ ÙˆØ§Ù„ØªØ°ÙƒÙŠØ± ÙˆØ§Ù„ØªØ£Ù†ÙŠØ« Ø³Ù„ÙŠÙ…Ø©ØŸ
â–¡ Ù‡Ù„ Ø§Ù„Ù†Øµ Ù…Ø­Ø§ÙŠØ¯ ÙˆÙ…Ù‡Ù†ÙŠØŸ
"""
        prompt_parts.append(format_instructions)
        
        # Add field extraction instructions if fields are provided
        if fields_to_extract:
            # Filter fields that need AI extraction
            ai_fields = [f for f in fields_to_extract 
                        if f.get('field_type') == 'extracted' 
                        and f.get('field_name')]
            
            if ai_fields:
                fields_prompt = ""
                for f in ai_fields:
                    field_name = f.get('field_name', '')
                    instructions = f.get('extraction_instructions', '').strip()
                    fields_prompt += f"\nâ€¢ {field_name}: {instructions or 'Ø§Ø³ØªØ®Ø±Ø¬ Ù…Ù† Ø§Ù„Ù†Øµ'}"
                
                extraction_instructions = f"""

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ù„Ù„ØªÙ„Ø®ÙŠØµØŒ Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„ØªØ§Ù„ÙŠØ© Ù…Ù† Ø§Ù„Ù†Øµ:
{fields_prompt}

âš ï¸ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:
Ø£Ø®Ø±Ø¬ Ø§Ù„Ø±Ø¯ Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ù„ÙŠ (JSON + Ø§Ù„ØªÙ„Ø®ÙŠØµ):

```json
{{
  "Ø§Ù„ØªÙ„Ø®ÙŠØµ": "Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù„Ø®Øµ Ù‡Ù†Ø§",
{chr(10).join([f'  "{f.get("field_name")}": "Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©"' + (',' if i < len(ai_fields)-1 else '') for i, f in enumerate(ai_fields)])}
}}
```

Ù…Ù„Ø§Ø­Ø¸Ø§Øª:
- Ø¶Ø¹ Ø§Ù„ØªÙ„Ø®ÙŠØµ ÙÙŠ Ø­Ù‚Ù„ "Ø§Ù„ØªÙ„Ø®ÙŠØµ"
- Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ù‚ÙŠÙ…Ø© Ù„Ø­Ù‚Ù„ Ù…Ø§ØŒ Ø§ØªØ±ÙƒÙ‡ ÙØ§Ø±ØºØ§Ù‹ ""
- Ø£Ø®Ø±Ø¬ JSON ØµØ­ÙŠØ­ ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù†Øµ Ø¥Ø¶Ø§ÙÙŠ
"""
                prompt_parts.append(extraction_instructions)
                
                prompt_parts.append(f"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{text}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â¬‡ï¸ Ø£Ø®Ø±Ø¬ JSON ÙÙ‚Ø· Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø£Ø¹Ù„Ø§Ù‡:
""")
            else:
                # No AI fields, regular output
                prompt_parts.append(f"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{text}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â¬‡ï¸ Ø£Ø®Ø±Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† Ø´Ø±Ø­ Ø£Ùˆ ØªØ¹Ù„ÙŠÙ‚Ø§Øª):
""")
        else:
            prompt_parts.append(f"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{text}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â¬‡ï¸ Ø£Ø®Ø±Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† Ø´Ø±Ø­ Ø£Ùˆ ØªØ¹Ù„ÙŠÙ‚Ø§Øª):
""")
        
        return "\n".join(prompt_parts)
    
    def _parse_combined_response(
        self,
        response: str,
        fields_to_extract: List[Dict[str, Any]]
    ) -> Tuple[str, Dict[str, Any]]:
        """
        Parse combined AI response containing both summary and extracted fields
        Returns tuple of (summary_text, extracted_fields)
        """
        import json
        import re
        
        extracted = {}
        summary = response  # Default to full response if parsing fails
        
        try:
            # Try to find JSON in response
            json_match = re.search(r'\{[\s\S]*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                parsed = json.loads(json_str)
                
                # Extract summary
                summary = parsed.get('Ø§Ù„ØªÙ„Ø®ÙŠØµ', '') or parsed.get('summary', '') or ''
                
                # Extract other fields with normalization
                for k, v in parsed.items():
                    if k in ['Ø§Ù„ØªÙ„Ø®ÙŠØµ', 'summary']: continue
                    
                    val = str(v).strip() if v is not None else ""
                    # Store original key
                    extracted[k] = val
                    
                    # Store normalized key (remove underscores and handle Arabic variants)
                    norm_k = k.replace('_', '').replace(' ', '').strip()
                    extracted[norm_k] = val
                    
                    # Handle specific Arabic field mappings
                    if 'Ù…Ø­Ø§ÙØ¸Ù‡' in norm_k or 'Ù…Ø­Ø§ÙØ¸Ø©' in norm_k or 'governorate' in norm_k:
                        extracted['Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©'] = val
                        extracted['Ø§Ù„Ù…Ø­Ø§ÙØ¸Ù‡'] = val
                        extracted['governorate'] = val
                        
                    if 'Ù…ØµØ¯Ø±' in norm_k or 'source' in norm_k:
                        extracted['Ø§Ù„Ù…ØµØ¯Ø±'] = val
                        extracted['source'] = val
                        
                    if 'ØªØµÙ†ÙŠÙ' in norm_k or 'classification' in norm_k or 'category' in norm_k:
                        extracted['Ø§Ù„ØªØµÙ†ÙŠÙ'] = val
                        extracted['category'] = val

                error_logger.log_info(f"[Pipeline] âœ… Parsed combined response | Summary: {len(summary)} chars | Fields: {len(extracted)}")
        except json.JSONDecodeError as e:
            error_logger.log_warning(f"[Pipeline] âš ï¸ Failed to parse JSON from response: {str(e)}")
            summary = response
        except Exception as e:
            error_logger.log_warning(f"[Pipeline] âš ï¸ Error parsing combined response: {str(e)}")
            summary = response
        
        return (summary, extracted)
    
    async def _get_training_examples(self, task_id: int) -> List[Dict]:
        """Get training examples for few-shot learning"""
        try:
            examples = await db.get_training_examples(task_id=task_id)
            if examples:
                active_examples = [e for e in examples if e.get('is_active', True)]
                
                # Increment use_count for each example used
                for example in active_examples[:5]:  # Only for the ones we'll actually use
                    try:
                        await db.increment_example_use_count(example['id'])
                    except Exception as count_err:
                        error_logger.log_info(f"[Pipeline] Failed to increment use_count for example {example['id']}: {str(count_err)}")
                
                return active_examples
        except Exception as e:
            error_logger.log_info(f"[Pipeline] No training examples for task {task_id}: {str(e)}")
        return []
    
    async def _get_config(self, task_id: int) -> Dict[str, Any]:
        """Get processing configuration for task"""
        try:
            config = await db.get_processing_config(task_id)
            if config:
                return config
        except Exception as e:
            error_logger.log_info(f"[Pipeline] No custom config for task {task_id}, using defaults")
        
        return {
            'enable_entity_extraction': True,
            'enable_sentiment_analysis': True,
            'enable_keyword_detection': True,
            'enable_output_validation': True,
            'enable_rule_verification': True,
            'preserve_formatting': True,
            'output_format': 'markdown',
            'temperature': '0.7',
            'quality_level': 'balanced'
        }
    
    def _empty_result(self, text: str) -> PipelineResult:
        """Return empty result for empty text"""
        return PipelineResult(
            original_text=text or "",
            final_text=text or "",
            stages=[],
            success=True
        )
    
    async def process_video_summary(
        self,
        transcript: str,
        task_id: int,
        provider: str,
        model: str,
        config: Optional[Dict[str, Any]] = None
    ) -> PipelineResult:
        """
        Process video transcript with specialized video summarization
        """
        video_system_prompt = """Ø£Ù†Øª Ù…Ø­Ø±Ø± ÙÙŠØ¯ÙŠÙˆ Ù…Ø­ØªØ±Ù Ù…ØªØ®ØµØµ ÙÙŠ ØªÙ„Ø®ÙŠØµ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª.
Ù…Ù‡Ù…ØªÙƒ: ØªÙ„Ø®ÙŠØµ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø´ÙƒÙ„ Ù…ÙˆØ¬Ø² ÙˆØ¯Ù‚ÙŠÙ‚ Ù…Ø¹:
- Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
- Ø°ÙƒØ± Ø§Ù„Ù…ØªØ­Ø¯Ø«ÙŠÙ† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠÙŠÙ† Ø¥Ù† ÙˆØ¬Ø¯ÙˆØ§
- ØªÙ„Ø®ÙŠØµ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨ØªØ±ØªÙŠØ¨ Ø²Ù…Ù†ÙŠ
- Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¹Ø§Ù… Ù„Ù„ÙÙŠØ¯ÙŠÙˆ"""
        
        video_rules = await db.get_task_rules(task_id)
        video_rules = [r for r in video_rules if r.get('type') == 'video_summarize' and r.get('is_active')]
        
        return await self.process(
            text=transcript,
            task_id=task_id,
            provider=provider,
            model=model,
            system_prompt=video_system_prompt,
            custom_rules=video_rules,
            config=config
        )
    
    async def process_audio_summary(
        self,
        transcript: str,
        task_id: int,
        provider: str,
        model: str,
        config: Optional[Dict[str, Any]] = None
    ) -> PipelineResult:
        """
        Process audio transcript with specialized audio summarization
        Applies AUDIO-SPECIFIC summarization rules (type='audio_summarize')
        Falls back to general summarization rules if no audio-specific rules exist
        """
        transcript_length = len(transcript)
        
        if transcript_length < 200:
            target_length = "50-100 ÙƒÙ„Ù…Ø©"
        elif transcript_length < 500:
            target_length = "100-150 ÙƒÙ„Ù…Ø©"
        elif transcript_length < 1000:
            target_length = "150-250 ÙƒÙ„Ù…Ø©"
        elif transcript_length < 3000:
            target_length = "250-400 ÙƒÙ„Ù…Ø©"
        else:
            target_length = "400-600 ÙƒÙ„Ù…Ø©"
        
        audio_system_prompt = f"""Ø£Ù†Øª Ù…Ø­Ø±Ø± ØµÙˆØªÙŠ Ù…Ø­ØªØ±Ù Ù…ØªØ®ØµØµ ÙÙŠ ØªÙ„Ø®ÙŠØµ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ© ÙˆØ§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„ØµÙˆØªÙŠØ©.

âš ï¸ ØªØ¹Ù„ÙŠÙ…Ø§Øª ØµØ§Ø±Ù…Ø© - ÙŠØ¬Ø¨ Ø§ØªØ¨Ø§Ø¹Ù‡Ø§ Ø¨Ø¯Ù‚Ø©:

1. ğŸ“ Ù‚Ù… Ø¨Ø¥Ù†ØªØ§Ø¬ Ù…Ù„Ø®Øµ Ù…Ø®ØªØµØ± ÙˆÙ…ÙˆØ¬Ø² Ù„Ù„Ù†Øµ Ø§Ù„Ù…ÙÙØ±Ù‘Øº (ÙˆÙ„ÙŠØ³ Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„)
2. ğŸ“ Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù„Ù„Ù…Ù„Ø®Øµ: {target_length} (Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ {transcript_length} Ø­Ø±Ù)
3. ğŸ¯ Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙˆØ§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ù…Ù‡Ù…Ø© ÙÙ‚Ø·
4. âŒ Ù„Ø§ ØªÙØ¹ÙØ¯ ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ - Ù‚Ù… Ø¨ØªÙ„Ø®ÙŠØµÙ‡ ÙÙ‚Ø·
5. âœ… Ø£Ø¹Ø¯ Ø§Ù„Ù…Ù„Ø®Øµ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø¯ÙˆÙ† Ù…Ù‚Ø¯Ù…Ø§Øª Ø£Ùˆ Ø¹Ø¨Ø§Ø±Ø§Øª Ù…Ø«Ù„ "Ø¥Ù„ÙŠÙƒ Ø§Ù„Ù…Ù„Ø®Øµ"

ğŸ“‹ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ø¬ÙŠØ¯:
- Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙˆØ§Ù„Ø¬ÙˆÙ‡Ø±ÙŠØ©
- Ø­Ø°Ù Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª ÙˆØ§Ù„Ø­Ø´Ùˆ ÙˆØ§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ©
- Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¬Ù…Ù„ Ù‚ØµÙŠØ±Ø© ÙˆÙ…Ø¨Ø§Ø´Ø±Ø©
- Ø°ÙƒØ± Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ ÙˆØ§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„ØªÙˆØ§Ø±ÙŠØ® Ø§Ù„Ù…Ù‡Ù…Ø©
- Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¹Ø§Ù… ÙˆØ§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø£ØµÙ„ÙŠ

ğŸš« ØªØ¬Ù†Ø¨:
- Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù†Øµ ÙƒÙ…Ø§ Ù‡Ùˆ
- Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ
- Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ø¨Ø§Ø±Ø§Øª Ø·ÙˆÙŠÙ„Ø© ÙˆÙ…Ø¹Ù‚Ø¯Ø©
- Ø°ÙƒØ± ØªÙØ§ØµÙŠÙ„ ØºÙŠØ± Ø¶Ø±ÙˆØ±ÙŠØ©"""
        
        # First try to get audio-specific rules
        all_rules = await db.get_task_rules(task_id)
        audio_rules = [r for r in all_rules if r.get('type') == 'audio_summarize' and r.get('is_active')]
        
        # If no audio-specific rules exist, fall back to general summarization rules
        if not audio_rules:
            error_logger.log_info(f"[Pipeline] No audio-specific rules found, falling back to general summarization rules")
            audio_rules = [r for r in all_rules if r.get('type') == 'summarize' and r.get('is_active')]
        
        if audio_rules:
            error_logger.log_info(f"[Pipeline] ğŸ™ï¸ Loaded {len(audio_rules)} AUDIO-SPECIFIC summarization rules for task {task_id}")
        
        return await self.process(
            text=transcript,
            task_id=task_id,
            provider=provider,
            model=model,
            system_prompt=audio_system_prompt,
            custom_rules=audio_rules,
            config=config
        )

ai_pipeline = AIPipeline()
