"""
AI Processing Pipeline
Unified 4-stage AI processing system
Stage 1: Preprocessing (Entity extraction, sentiment analysis)
Stage 2: Rule Engine (Entity replacement, context neutralization)
Stage 3: AI Summarization (Enhanced prompts with rules)
Stage 4: Postprocessing (Validation, verification, formatting)
"""
import asyncio
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
from utils.error_handler import ErrorLogger
from utils.database import db
from services.ai_preprocessing import preprocessing_engine, PreprocessingResult
from services.ai_rule_engine import rule_engine, RuleEngineResult
from services.ai_postprocessing import postprocessing_engine, PostprocessingResult

error_logger = ErrorLogger("ai_pipeline")

@dataclass
class PipelineStageResult:
    """Result from a single pipeline stage"""
    stage_name: str
    input_text: str
    output_text: str
    processing_time: float
    success: bool
    details: Dict[str, Any] = field(default_factory=dict)
    error: Optional[str] = None

@dataclass
class PipelineResult:
    """Complete pipeline processing result"""
    original_text: str
    final_text: str
    stages: List[PipelineStageResult]
    preprocessing: Optional[PreprocessingResult] = None
    rule_engine: Optional[RuleEngineResult] = None
    postprocessing: Optional[PostprocessingResult] = None
    total_time: float = 0.0
    quality_score: float = 1.0
    success: bool = True
    rules_applied_count: int = 0
    entities_replaced: Dict[str, List[Tuple[str, str]]] = field(default_factory=dict)
    warnings: List[str] = field(default_factory=list)
    errors: List[str] = field(default_factory=list)
    extracted_fields: Dict[str, Any] = field(default_factory=dict)

class AIPipeline:
    """
    Main AI Processing Pipeline
    Orchestrates all 4 stages of AI text processing
    """
    
    def __init__(self):
        self.preprocessing = preprocessing_engine
        self.rule_engine = rule_engine
        self.postprocessing = postprocessing_engine
        self.ai_manager = None
        error_logger.log_info("[Pipeline] AI Processing Pipeline initialized")
    
    def set_ai_manager(self, ai_manager):
        """Set the AI manager for text generation"""
        self.ai_manager = ai_manager
    
    async def process(
        self,
        text: str,
        task_id: int,
        provider: str,
        model: str,
        system_prompt: Optional[str] = None,
        custom_rules: Optional[List[Dict]] = None,
        config: Optional[Dict[str, Any]] = None,
        video_source_info: Optional[Dict[str, str]] = None,
        fields_to_extract: Optional[Any] = None,
        serial_number: Optional[int] = None
    ) -> PipelineResult:
        """
        Main pipeline processing function
        Executes all 4 stages in order
        """
        start_time = datetime.now()
        config = config or await self._get_config(task_id)
        
        # If fields_to_extract is True, get them from the template
        if fields_to_extract is True:
            template = await db.get_task_publishing_template(task_id)
            fields_to_extract = template.get("fields", []) if template else []
        
        if not text or not text.strip():
            return self._empty_result(text)
        
        stages = []
        warnings = []
        errors = []
        current_text = text.strip()
        
        error_logger.log_info(f"[Pipeline] โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ")
        error_logger.log_info(f"[Pipeline] ๐ STARTING PROCESSING | Task: {task_id} | Text length: {len(current_text)}")
        error_logger.log_info(f"[Pipeline] ๐ฅ INPUT TEXT: {current_text[:200]}...")
        error_logger.log_info(f"[Pipeline] โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ")
        
        stage_start = datetime.now()
        preprocessing_result = await self.preprocessing.process(current_text, config)
        stage_time = (datetime.now() - stage_start).total_seconds()
        
        stages.append(PipelineStageResult(
            stage_name='preprocessing',
            input_text=current_text[:100],
            output_text=preprocessing_result.cleaned_text[:100],
            processing_time=stage_time,
            success=True,
            details={
                'entities_found': len(preprocessing_result.entities),
                'sentiment': preprocessing_result.sentiment.overall,
                'keywords': len(preprocessing_result.keywords),
                'language': preprocessing_result.language
            }
        ))
        
        error_logger.log_info(f"[Pipeline] Stage 1 complete | Entities: {len(preprocessing_result.entities)} | Sentiment: {preprocessing_result.sentiment.overall}")
        
        stage_start = datetime.now()
        rule_result = await self.rule_engine.process(
            current_text,
            task_id,
            preprocessing_result
        )
        stage_time = (datetime.now() - stage_start).total_seconds()
        
        current_text = rule_result.processed_text
        
        stages.append(PipelineStageResult(
            stage_name='rule_engine',
            input_text=text[:100],
            output_text=current_text[:100],
            processing_time=stage_time,
            success=rule_result.success,
            details={
                'rules_applied': len(rule_result.rules_applied),
                'replacements': rule_result.total_replacements,
                'context_mods': len(rule_result.context_modifications)
            },
            error='; '.join(rule_result.errors) if rule_result.errors else None
        ))
        
        warnings.extend(rule_result.errors)
        
        error_logger.log_info(f"[Pipeline] Stage 2 complete | Rules: {len(rule_result.rules_applied)} | Replacements: {rule_result.total_replacements}")
        
        # โ Log detailed rules applied
        if rule_result.rules_applied:
            error_logger.log_info(f"[Pipeline] ๐ DETAILED RULES APPLIED:")
            for i, rule_app in enumerate(rule_result.rules_applied, 1):
                error_logger.log_info(f"[Pipeline]   Rule {i}: {rule_app.rule_name} (Type: {rule_app.rule_type})")
                if rule_app.changes_made:
                    error_logger.log_info(f"[Pipeline]     Changes: {len(rule_app.changes_made)} modifications")
        
        # โ Log text after rules processing
        error_logger.log_info(f"[Pipeline] ๐ TEXT AFTER RULES: {current_text[:300]}...")
        
        # Skip pre-truncation summarization rule - let AI do the actual summarization
        # The AI will handle both summarization and rule application via custom_rules in prompt
        
        stage_start = datetime.now()
        ai_output, extracted_fields = await self._process_with_ai(
            current_text,
            task_id,
            provider,
            model,
            system_prompt,
            custom_rules,
            preprocessing_result,
            rule_result,
            config,
            video_source_info=video_source_info,
            fields_to_extract=fields_to_extract,
            serial_number=serial_number
        )
        stage_time = (datetime.now() - stage_start).total_seconds()
        
        if ai_output and ai_output.strip():
            stages.append(PipelineStageResult(
                stage_name='ai_summarization',
                input_text=current_text[:100],
                output_text=ai_output[:100],
                processing_time=stage_time,
                success=True,
                details={
                    'provider': provider,
                    'model': model,
                    'input_length': len(current_text),
                    'output_length': len(ai_output)
                }
            ))
            error_logger.log_info(f"[Pipeline] โ Stage 3 AI SUCCESS | Input: {len(current_text)} โ Output: {len(ai_output)} chars")
            error_logger.log_info(f"[Pipeline] ๐ค AI OUTPUT: {ai_output[:300]}...")
            current_text = ai_output
        else:
            stages.append(PipelineStageResult(
                stage_name='ai_summarization',
                input_text=current_text[:100],
                output_text=current_text[:100],
                processing_time=stage_time,
                success=False,
                error='AI returned empty response'
            ))
            error_logger.log_warning(f"[Pipeline] โ๏ธ Stage 3 AI FAILED | Empty response from {provider}/{model}")
            error_logger.log_warning(f"[Pipeline] โ๏ธ Using original text (no summarization applied)")
            warnings.append('AI summarization failed, using rule-processed text')
        
        error_logger.log_info(f"[Pipeline] Stage 3 complete | Output length: {len(current_text)}")
        
        stage_start = datetime.now()
        postprocess_result = await self.postprocessing.process(
            current_text,
            text,
            rule_result,
            config
        )
        stage_time = (datetime.now() - stage_start).total_seconds()
        
        final_text = postprocess_result.final_output
        
        stages.append(PipelineStageResult(
            stage_name='postprocessing',
            input_text=current_text[:100],
            output_text=final_text[:100],
            processing_time=stage_time,
            success=postprocess_result.success,
            details={
                'validations': len(postprocess_result.validations),
                'rules_verified': sum(1 for v in postprocess_result.rules_verified.values() if v),
                'quality_score': postprocess_result.quality_score
            }
        ))
        
        warnings.extend(postprocess_result.warnings)
        errors.extend(postprocess_result.errors)
        
        error_logger.log_info(f"[Pipeline] Stage 4 complete | Quality: {postprocess_result.quality_score:.2f}")
        
        total_time = (datetime.now() - start_time).total_seconds()
        
        result = PipelineResult(
            original_text=text,
            final_text=final_text,
            stages=stages,
            preprocessing=preprocessing_result,
            rule_engine=rule_result,
            postprocessing=postprocess_result,
            total_time=total_time,
            quality_score=postprocess_result.quality_score,
            success=all(s.success for s in stages),
            rules_applied_count=len(rule_result.rules_applied),
            entities_replaced=rule_result.entities_replaced,
            warnings=warnings,
            errors=errors,
            extracted_fields=extracted_fields or {}
        )
        
        error_logger.log_info(
            f"[Pipeline] โ COMPLETE | Total time: {total_time:.2f}s | "
            f"Quality: {postprocess_result.quality_score:.2f} | "
            f"Original: {len(text)} โ Final: {len(final_text)} chars"
        )
        
        reduction_pct = 100 - (len(final_text) * 100 // len(text)) if len(text) > 0 else 0
        error_logger.log_info(
            f"[Pipeline] ๐ SUMMARY: {len(text)} โ {len(final_text)} chars ({reduction_pct}% reduction) | "
            f"Rules: {len(rule_result.rules_applied)} | Entities: {len(rule_result.entities_replaced)}"
        )
        
        return result
    
    async def _process_with_ai(
        self,
        text: str,
        task_id: int,
        provider: str,
        model: str,
        system_prompt: Optional[str],
        custom_rules: Optional[List[Dict]],
        preprocessing_result: PreprocessingResult,
        rule_result: RuleEngineResult,
        config: Dict[str, Any],
        video_source_info: Optional[Dict[str, str]] = None,
        fields_to_extract: Optional[List[Dict[str, Any]]] = None,
        serial_number: Optional[int] = None
    ) -> Tuple[str, Dict[str, Any]]:
        """
        Stage 3: Process text with AI
        Builds enhanced prompt with context from preprocessing and rules
        Includes training examples for few-shot learning
        """
        if not self.ai_manager:
            from services.ai_providers import ai_manager
            self.ai_manager = ai_manager
        
        # โ Log input text for AI processing
        error_logger.log_info(f"[Pipeline] โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ")
        error_logger.log_info(f"[Pipeline] ๐ค STAGE 3: AI SUMMARIZATION")
        error_logger.log_info(f"[Pipeline] ๐ฅ AI INPUT TEXT ({len(text)} chars):")
        error_logger.log_info(f"[Pipeline] {text[:500]}...")
        error_logger.log_info(f"[Pipeline] โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ")
        
        training_examples = await self._get_training_examples(task_id)
        error_logger.log_info(f"[Pipeline] ๐ Training Examples | Loaded: {len(training_examples)} for task {task_id}")
        if training_examples:
            for i, ex in enumerate(training_examples[:3], 1):
                error_logger.log_info(f"[Pipeline]   ๐ Example {i}: type={ex.get('example_type')} | input={ex.get('input_text', '')[:50]}... | output={ex.get('expected_output', '')[:50]}...")
        
        # No token limit - let AI produce complete summary without cutoff
        max_tokens = 128000  # No limit - allows full text generation without any restrictions
        if custom_rules:
            error_logger.log_info(f"[Pipeline] โ๏ธ SUMMARIZATION RULES ({len(custom_rules)} rules):")
            for i, rule in enumerate(custom_rules, 1):
                rule_name = rule.get('name', 'Unknown')
                rule_type = rule.get('type', 'unknown')
                rule_prompt = rule.get('prompt', '')
                rule_config = rule.get('config', {})
                error_logger.log_info(f"[Pipeline]   ๐ Rule {i}: name={rule_name} | type={rule_type}")
                error_logger.log_info(f"[Pipeline]      prompt={rule_prompt[:100]}...")
                error_logger.log_info(f"[Pipeline]      config={str(rule_config)[:100]}...")
        else:
            error_logger.log_info(f"[Pipeline] โ๏ธ SUMMARIZATION RULES: None provided")
        
        # Initialize extracted fields with serial number
        extracted_fields = {}
        if serial_number is not None:
            serial_val = f"#{serial_number}"
            extracted_fields["ุฑูู_ุงูููุฏ"] = serial_val
            extracted_fields["ุฑูู_ุงูููุฏ_"] = serial_val
            extracted_fields["serial_number"] = serial_number
            extracted_fields["record_number"] = serial_number
            error_logger.log_info(f"[Pipeline] ๐ Injected serial number: {serial_val}")
        
        # Build enhanced prompt including field extraction if needed
        error_logger.log_info(f"[Pipeline] ๐ BUILDING ENHANCED PROMPT...")
        enhanced_prompt = self._build_enhanced_prompt(
            text,
            system_prompt,
            custom_rules,
            preprocessing_result,
            rule_result,
            config,
            training_examples,
            video_source_info=video_source_info,
            fields_to_extract=fields_to_extract
        )
        
        error_logger.log_info(f"[Pipeline] โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ")
        error_logger.log_info(f"[Pipeline] ๐ AI PROMPT ({len(enhanced_prompt)} chars):")
        error_logger.log_info(f"[Pipeline] {enhanced_prompt[:800]}...")
        error_logger.log_info(f"[Pipeline] โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ")
        
        temperature = float(config.get('temperature', '0.7'))
        quality = config.get('quality_level', 'balanced')
        
        # No token restrictions - allow full text completion
        error_logger.log_info(f"[Pipeline] ๐ Token limit: UNLIMITED (max_tokens={max_tokens} - ุจุฏูู ุญุฏูุฏ)")
        
        try:
            error_logger.log_info(f"[Pipeline] ๐ CALLING AI")
            error_logger.log_info(f"[Pipeline]    Provider: {provider}")
            error_logger.log_info(f"[Pipeline]    Model: {model}")
            error_logger.log_info(f"[Pipeline]    Max Tokens: {max_tokens}")
            error_logger.log_info(f"[Pipeline]    Temperature: {temperature}")
            
            result = await self.ai_manager.generate(
                provider=provider,
                model=model,
                prompt=enhanced_prompt,
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            if not result:
                error_logger.log_warning(f"[Pipeline] โ๏ธ AI returned EMPTY result (provider={provider}, model={model})")
                return ("", extracted_fields)
            
            error_logger.log_info(f"[Pipeline] โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ")
            error_logger.log_info(f"[Pipeline] โ AI RETURNED {len(result)} chars")
            error_logger.log_info(f"[Pipeline] ๐ค AI OUTPUT (BEFORE RULES):")
            error_logger.log_info(f"[Pipeline] {result[:500]}...")
            error_logger.log_info(f"[Pipeline] โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ")
            
            # Parse result - check if it contains JSON with extracted fields
            if fields_to_extract:
                summary_text, ai_extracted = self._parse_combined_response(result, fields_to_extract)
                extracted_fields.update(ai_extracted)
                error_logger.log_info(f"[Pipeline] ๐ฆ Extracted {len(ai_extracted)} fields in single AI call")
                
                # โ FIXED: Apply summarization rules to extracted summary
                summary_text = await self._apply_post_summarization_rules(summary_text, task_id)
                error_logger.log_info(f"[Pipeline] โ Post-summarization rules applied | Final: {len(summary_text)} chars")
                
                return (summary_text, extracted_fields)
            
            # โ FIXED: Apply summarization rules to direct AI output
            final_result = await self._apply_post_summarization_rules(result, task_id)
            error_logger.log_info(f"[Pipeline] โ Post-summarization rules applied | Final: {len(final_result)} chars")
            error_logger.log_info(f"[Pipeline] ๐ค FINAL OUTPUT (AFTER RULES): {final_result[:500]}...")
            
            return (final_result or "", extracted_fields)
        except Exception as e:
            error_logger.log_warning(f"[Pipeline] โ AI generation EXCEPTION: {str(e)} (provider={provider}, model={model})")
            import traceback
            error_logger.log_warning(f"[Pipeline] Traceback: {traceback.format_exc()}")
            return ("", extracted_fields)
    
    async def _apply_post_summarization_rules(self, text: str, task_id: int) -> str:
        """
        โ FIXED: Apply summarization rules to AI output
        Supports maxLength, style, keyPointsCount from database rules
        """
        try:
            if not text:
                return text
            
            from utils.database import db
            ai_rules = await db.get_task_rules(task_id)
            if not ai_rules:
                return text
            
            # Filter for active summarize rules
            summarize_rules = []
            for r in ai_rules:
                rule_dict = dict(r) if hasattr(r, '__iter__') and not isinstance(r, dict) else r
                if isinstance(rule_dict, dict) and rule_dict.get('type') == 'summarize' and rule_dict.get('is_active'):
                    summarize_rules.append(rule_dict)
            
            if not summarize_rules:
                return text
            
            processed_text = text
            for rule in sorted(summarize_rules, key=lambda r: r.get('priority', 0), reverse=True):
                rule_config = rule.get('config', {})
                if isinstance(rule_config, str):
                    try:
                        import json
                        rule_config = json.loads(rule_config)
                    except:
                        rule_config = {}
                
                # Extract max_length and apply truncation
                max_length = rule_config.get('maxLength') or rule_config.get('max_length')
                if max_length and isinstance(max_length, (int, float)) and max_length > 0:
                    if len(processed_text) > max_length:
                        processed_text = processed_text[:int(max_length)].rsplit(' ', 1)[0]
                        if not processed_text.endswith(('...', 'ุ', '.', 'ุ')):
                            processed_text += '...'
                        error_logger.log_info(f"[Pipeline] โ Applied max_length({max_length}): {len(text)} โ {len(processed_text)} chars")
            
            return processed_text
        except Exception as e:
            error_logger.log_warning(f"[Pipeline] โ๏ธ Error in post-summarization rules: {str(e)}")
            return text
    
    def _build_enhanced_prompt(
        self,
        text: str,
        system_prompt: Optional[str],
        custom_rules: Optional[List[Dict]],
        preprocessing_result: PreprocessingResult,
        rule_result: RuleEngineResult,
        config: Dict[str, Any],
        training_examples: Optional[List[Dict]] = None,
        video_source_info: Optional[Dict[str, str]] = None,
        fields_to_extract: Optional[List[Dict[str, Any]]] = None
    ) -> str:
        """
        Build enhanced prompt with all context, rules, and training examples
        Includes video metadata (title, description, uploader) for better summarization
        """
        prompt_parts = []
        
        base_prompt = system_prompt or """ุฃูุช ูุญุฑุฑ ุฅุฎุจุงุฑู ูุญุชุฑู ููุญูู ูุบูู ุฐูู ูุชุฎุตุต ูู ุชุญุฑูุฑ ูุชูุฎูุต ุงูุฃุฎุจุงุฑ ูุงูุชูุงุฑูุฑ.

๐ฏ ููุงูู ุงูุฃุณุงุณูุฉ (ุจุงูุชุฑุชูุจ):
1. ุชุทุจูู ููุงุนุฏ ุงูุงุณุชุจุฏุงู ุงูุฐูู (ุฅุฐุง ููุฌุฏุช) - ูุฐู ุงูุฃููููุฉ ุงููุตูู
2. ุงุณุชุฎุฑุงุฌ ุงูุญููู ุงููุทููุจุฉ ุจุฏูุฉ ูู ูุงูู ุงููุต ุงููุฑูู (ุจูุง ูู ุฐูู ุงููุงุจุดู ูุงููุต ุงูููุฑุบ)
3. ุชูุฎูุต ุงููุต ุจุดูู ููุฌุฒ ูุฏููู (ููุงุญุธุฉ ูุงูุฉ: ูุง ุชูุฑุฑ ุงูุญููู ุงููุณุชุฎุฑุฌุฉ ูุซู ุงููุญุงูุธุฉ ูุงููุตุฏุฑ ุฏุงุฎู ูุต ุงูุชูุฎูุต ููุณู)
4. ุงูุญูุงุธ ุนูู ุงูุญูุงุฏ ูุงูููุถูุนูุฉ
5. ุงุณุชุฎุฏุงู ุตูุงุบุฉ ุฑุณููุฉ ูููููุฉ

๐ ุชุนูููุงุช ุฎุงุตุฉ ุจุงูุงุณุชุฎุฑุงุฌ:
โข ุงุณุชุฎุฑุฌ "ุงููุญุงูุธุฉ" ูู ุงููุต ุฅุฐุง ุฐูุฑุช ุฃู ูุฏููุฉ ุฃู ููุทูุฉ ููููุฉ (ูุซู: ุนุฏูุ ุตูุนุงุกุ ูุฃุฑุจุ ุชู ุฃุจูุจุ ูุฃุฑุจุ ุชุนุฒุ ุฅูุฎ). ุฅุฐุง ูู ุชุฐูุฑ ูุญุงูุธุฉุ ุงุชุฑู ุงูุญูู ูุงุฑุบุงู.
โข ุงุณุชุฎุฑุฌ "ุงููุตุฏุฑ" ุจูุงุกู ุนูู ุงูุฌูุฉ ุงููุงููุฉ ููุฎุจุฑ ุฃู ุงูุฃุดุฎุงุต ุงููุชุญุฏุซูู ุฃู ุงูุณูุงู ุงููุฐููุฑ ูู ุงููุต ุฃู ุงููุงุจุดู.
โข ุงุจุญุซ ูู ูุงูู ุงููุต ุงููุฏููุฌ (ุงููุงุจุดู + ูุญุชูู ุงูููุฏูู) ุนู ูุฐู ุงูุชูุงุตูู."""
        
        prompt_parts.append(base_prompt)
        
        # Add video metadata if available (title, description, uploader)
        if video_source_info:
            metadata_text = "\n\n๐น ูุนูููุงุช ุงูููุฏูู/ุงููุญุชูู:"
            
            if video_source_info.get('title'):
                metadata_text += f"\nโข ุงูุนููุงู: {video_source_info.get('title')}"
            
            if video_source_info.get('description'):
                desc = video_source_info.get('description', '')
                # Truncate if too long
                if len(desc) > 500:
                    desc = desc[:500] + "..."
                metadata_text += f"\nโข ุงููุตู: {desc}"
            
            if video_source_info.get('uploader'):
                metadata_text += f"\nโข ุงููุตุฏุฑ/ุงููุญูู: {video_source_info.get('uploader')}"
            
            if video_source_info.get('platform'):
                metadata_text += f"\nโข ุงูููุตุฉ: {video_source_info.get('platform')}"
            
            if video_source_info.get('duration'):
                duration = video_source_info.get('duration', 0)
                if isinstance(duration, (int, float)) and duration > 0:
                    metadata_text += f"\nโข ุงููุฏุฉ: {int(duration)} ุซุงููุฉ"
            
            metadata_text += "\n\nโ๏ธ ุงุณุชุฎุฏู ูุฐู ุงููุนูููุงุช ูุชุญุณูู ูููู ูููุญุชูู ูุงูุชูุฎูุต ุงูุฏููู"
            prompt_parts.append(metadata_text)
        
        # Add summarization options from config
        summ_config = config.get('config', {}) if isinstance(config.get('config'), dict) else {}
        max_length = summ_config.get('maxLength', 300)
        style = summ_config.get('style', 'balanced')
        key_points = summ_config.get('keyPointsCount', 3)
        
        if max_length or style:
            options_text = f"\n\nุฎูุงุฑุงุช ุงูุชูุฎูุต:"
            options_text += f"\n- ุงูุทูู ุงูุฃูุตู: {max_length} ุญุฑู"
            options_text += f"\n- ุงูุฃุณููุจ: {style}"
            if key_points:
                options_text += f"\n- ุนุฏุฏ ุงูููุงุท ุงูุฑุฆูุณูุฉ: {key_points}"
            prompt_parts.append(options_text)
        
        if custom_rules:
            rules_text = "\n\nโ๏ธ ุงูููุงุนุฏ ุงูุฅูุฒุงููุฉ (ูุฌุจ ุชุทุจูููุง ุจุฏูุฉ):"
            for i, rule in enumerate(custom_rules, 1):
                rule_prompt = rule.get('prompt', '')
                rule_type = rule.get('type', '')
                
                # For summarization rules, build enhanced prompt from config
                if rule_type == 'summarize' and not rule_prompt:
                    rule_config = rule.get('config', {})
                    
                    # Parse config if it's a JSON string
                    if isinstance(rule_config, str):
                        try:
                            import json
                            rule_config = json.loads(rule_config)
                        except:
                            rule_config = {}
                    
                    max_length = rule_config.get('maxLength', 300) if isinstance(rule_config, dict) else 300
                    style = rule_config.get('style', 'balanced') if isinstance(rule_config, dict) else 'balanced'
                    rule_prompt = f"ูู ุจุชูุฎูุต ุงููุต ููููู {style} ูุน ุงูุญูุงุธ ุนูู ุงููุนูู ุงูุฃุณุงุณู (ุงูุญุฏ ุงูุฃูุตู: {max_length} ุญุฑู)"
                
                if rule_prompt:
                    rules_text += f"\n{i}. {rule_prompt}"
                    
                    # Add special handling for news format requirements
                    if 'ุฎุจุฑ' in rule_prompt or 'ุฃุณููุจ ุฅุฎุจุงุฑู' in rule_prompt or 'news' in rule_prompt.lower():
                        rules_text += "\n   ๐ฐ ุชูุณูู ุงูุฎุจุฑ ุงูุฅูุฒุงูู:"
                        rules_text += "\n   โข ุงูุชุจ ุงูุฎุจุฑ ูู ุดูู ููุฑุฉ ูุงุญุฏุฉ ูุชุณูุณูุฉ"
                        rules_text += "\n   โข ุงุจุฏุฃ ุจุฃูู ุงููุนูููุงุช (ุงูููุฑุฉ ุงูุฑุฆูุณูุฉ)"
                        rules_text += "\n   โข ูุง ุชุณุชุฎุฏู ุงูููุงุท (โข) ุฃู ุงูุชุฑููู"
                        rules_text += "\n   โข ุญุงูุธ ุนูู ุงูุชุณูุณู ุงูููุทูู ูุงูุณูุงุณุฉ"
                        rules_text += "\n   โข ุงุฌุนู ุงูุตูุงุบุฉ ุงุญุชุฑุงููุฉ ูุฅุฎุจุงุฑูุฉ"
                    
            
            if len(rules_text) > len("\n\nโ๏ธ ุงูููุงุนุฏ ุงูุฅูุฒุงููุฉ (ูุฌุจ ุชุทุจูููุง ุจุฏูุฉ):"):
                prompt_parts.append(rules_text)
        
        if rule_result.entities_replaced:
            entity_instructions = """

๐ ุงูุงุณุชุจุฏุงูุงุช ุงูุชู ุชู ุชุทุจูููุง ูุณุจูุงู (ุชุฃูุฏ ูู ุนุฏู ุฅุนุงุฏุชูุง ููุฃุตู):
"""
            for entity_type, replacements in rule_result.entities_replaced.items():
                entity_instructions += f"\n๐ ููุน ุงูููุงู: {entity_type}"
                for original, replacement in replacements:
                    entity_instructions += f"\n   โข '{original}' โ ุชู ุงุณุชุจุฏุงููุง ุจู โ '{replacement}'"
                    entity_instructions += f"\n     โก ุชุฃูุฏ ูู ุงุณุชุจุฏุงู ุฃู ุตูุบุฉ ุฃุฎุฑู ูู '{original}' ุจู '{replacement}'"
            
            entity_instructions += """

โ๏ธ ููุงุญุธุฉ ูููุฉ: ุฅุฐุง ูุฌุฏุช ุฃู ุฐูุฑ ุขุฎุฑ ูููููุงุช ุงูุฃุตููุฉ ูู ูุชู ุงุณุชุจุฏุงููุ ูู ุจุงุณุชุจุฏุงูู.
"""
            prompt_parts.append(entity_instructions)
        
        # Add semantic replacement rules for AI to find and replace variations
        if hasattr(rule_result, 'semantic_replacement_rules') and rule_result.semantic_replacement_rules:
            semantic_instructions = """

โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ๏ธ ูููุฉ ุงูุงุณุชุจุฏุงู ุงูุฐูู (ุงูุฃููููุฉ ุงููุตูู - ูุฌุจ ุชูููุฐูุง ุจุฏูุฉ ูุชูุงููุฉ)
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

๐ ุงููุทููุจ ููู:
ูู ุจุชุญููู ุงููุต ุงูุชุงูู ุณุทุฑุงู ุจุณุทุฑุ ูููุฉ ุจูููุฉุ ูุงุจุญุซ ุนู ุฃู ุฐูุฑ ููููุงูุงุช ุงููุญุฏุฏุฉ ุฃุฏูุงู.
ูุฌุจ ุงุณุชุจุฏุงููุง ุญุชู ูู ูุงูุช ููุชูุจุฉ ุจุทุฑููุฉ ูุฎุชููุฉ ุฃู ูู ุณูุงู ูุฎุชูู.

๐ ุทุฑููุฉ ุงูุชุญููู ุงููุทููุจุฉ:
1๏ธโฃ ุงูุชุญููู ุงูุณูุงูู: ุงููู ูุนูู ูู ุฌููุฉ ูุญุฏุฏ ูุง ุฅุฐุง ูุงูุช ุชุดูุฑ ููููุงู ุงููุทููุจ ุงุณุชุจุฏุงูู
2๏ธโฃ ุงูุชุญููู ุงูุฏูุงูู: ุงุจุญุซ ุนู ุงููุชุฑุงุฏูุงุช ูุงููููุงุช ุฐุงุช ุงููุนูู ุงููุดุงุจู
3๏ธโฃ ุงูุชุญููู ุงูุตุฑูู: ุงุจุญุซ ุนู ุฌููุน ุชุตุฑููุงุช ุงููููุฉ (ุฌูุนุ ููุฑุฏุ ูุคูุซุ ูุฐูุฑุ ูุถุงู)
4๏ธโฃ ุงูุชุญููู ุงูุฅููุงุฆู: ุชุนุฑู ุนูู ุงููููุฉ ุญุชู ูุน ุฃุฎุทุงุก ุฅููุงุฆูุฉ ุฃู ุงุฎุชูุงู ูู ุงูููุฒุงุช
5๏ธโฃ ุชุญููู ุงูุฅุดุงุฑุงุช: ุฅุฐุง ูุงู ุงููุต ูุดูุฑ ููููุงู ุจุถููุฑ ุฃู ููุจ ุฃู ูุตูุ ุงุณุชุจุฏูู ุฃูุถุงู

๐ ุฃููุงุน ุงููุทุงุจูุฉ ุงูุชู ูุฌุจ ุงูุจุญุซ ุนููุง:
โข ุงููุทุงุจูุฉ ุงูุญุฑููุฉ: ููุณ ุงููููุฉ ุจุงูุถุจุท
โข ุงููุทุงุจูุฉ ุงูุตุฑููุฉ: ุงูุฌูุน ูุงูููุฑุฏ (ูููุดูุง/ูููุดูุงุชุ ุญูุซู/ุญูุซููู/ุญูุซููู)
โข ุงููุทุงุจูุฉ ูุน ุงูุชุนุฑูู: ูุน "ุงู" ุฃู ุจุฏูููุง (ุงูุญูุซู/ุญูุซูุ ุงูุฌูุด/ุฌูุด)
โข ุงููุทุงุจูุฉ ูุน ุงูุถูุงุฆุฑ: ุงููููุฉ ูุน ุถูุงุฆุฑ ูุชุตูุฉ (ุฌูุดูุ ุฌูุดููุ ูููุดูุงุชูู)
โข ุงููุทุงุจูุฉ ุงูุณูุงููุฉ: ุนูุฏูุง ููุดุงุฑ ููููุงู ุจูุตู ุฃู ููุจ ูุนุฑูู
โข ุงููุทุงุจูุฉ ุงูุฌุฒุฆูุฉ: ุฅุฐุง ูุงูุช ุงููููุฉ ุฌุฒุกุงู ูู ุนุจุงุฑุฉ ุฃุทูู
โข ุงููุทุงุจูุฉ ุงูุฅููุงุฆูุฉ: ููุณ ุงููููุฉ ุจุฃุฎุทุงุก ุฅููุงุฆูุฉ ุดุงุฆุนุฉ

๐ฏ ููุงุนุฏ ุงูุงุณุชุจุฏุงู ุงูุฅูุฒุงููุฉ:
"""
            for rule in rule_result.semantic_replacement_rules:
                originals = rule.get('originals', [])
                replacements = rule.get('replacements', [])
                if not originals and rule.get('original'):
                    originals = [rule.get('original')]
                if not replacements and rule.get('replacement'):
                    replacements = [rule.get('replacement')]
                
                if originals and replacements:
                    primary_replacement = replacements[0] if replacements[0] else ""
                    originals_str = 'ุ '.join([str(o) for o in originals if o])
                    
                    semantic_instructions += f"""
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ ๐ ุงููููุงุช ุงูุฃุตููุฉ: {originals_str}
โ โ ุงูุจุฏูู ุงููุทููุจ: {primary_replacement}
โ 
โ ๐ ุชุนูููุงุช ุฎุงุตุฉ ุจูุฐู ุงููุงุนุฏุฉ:
โ โข ุงุจุญุซ ุนู ุฃู ุฐูุฑ ูุจุงุดุฑ ุฃู ุบูุฑ ูุจุงุดุฑ ููุฐู ุงููููุงุช
โ โข ุงุณุชุจุฏู ุฌููุน ุงูุชุตุฑููุงุช: (ููุฑุฏ/ุฌูุน/ูุฐูุฑ/ูุคูุซ/ูุนุฑู/ููุฑุฉ)
โ โข ุงุณุชุจุฏู ุญุชู ูู ูุงูุช ูุน ุถูุงุฆุฑ ูุชุตูุฉ ุฃู ุญุฑูู ุฌุฑ
โ โข ุฅุฐุง ูุงู ุงูุณูุงู ูุดูุฑ ูููุณ ุงูููุงู ุจุทุฑููุฉ ูุฎุชููุฉุ ุงุณุชุจุฏูู
โ โข ุญุงูุธ ุนูู ุณูุงุณุฉ ุงููุต ูุตุญุฉ ุงูุฅุนุฑุงุจ ุจุนุฏ ุงูุงุณุชุจุฏุงู
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
"""
            
            semantic_instructions += """
โก ุฃูุซูุฉ ุนูู ุงูุชุญููู ุงูุฐูู:
ูุซุงู 1: ุฅุฐุง ูุงู ุงููุทููุจ ุงุณุชุจุฏุงู "ูููุดูุง ุงูุญูุซู" ุจู "ุฌูุงุนุฉ ุฃูุตุงุฑ ุงููู"
   - "ูููุดูุง ุงูุญูุซู" โ "ุฌูุงุนุฉ ุฃูุตุงุฑ ุงููู" โ
   - "ูููุดูุงุช ุงูุญูุซู" โ "ุฌูุงุนุฉ ุฃูุตุงุฑ ุงููู" โ
   - "ุงูููููุดูุง ุงูุญูุซูุฉ" โ "ุฌูุงุนุฉ ุฃูุตุงุฑ ุงููู" โ
   - "ูููุดูุงุชูู" โ "ุงูุฌูุงุนุฉ" โ
   - "ุงููููุดูุง" (ุฅุฐุง ูุงู ุงูุณูุงู ูุดูุฑ ููุญูุซู) โ "ุฌูุงุนุฉ ุฃูุตุงุฑ ุงููู" โ

ูุซุงู 2: ุฅุฐุง ูุงู ุงููุทููุจ ุงุณุชุจุฏุงู "ุงูุฅุฑูุงุจููู" ุจู "ุงููุณูุญูู"
   - "ุงูุฅุฑูุงุจููู" โ "ุงููุณูุญูู" โ
   - "ุฅุฑูุงุจู" โ "ูุณูุญ" โ
   - "ุงูุฅุฑูุงุจููู" โ "ุงููุณูุญูู" โ
   - "ุฅุฑูุงุจูุฉ" โ "ูุณูุญุฉ" โ
   - "ุงูุฌูุงุนุฉ ุงูุฅุฑูุงุจูุฉ" โ "ุงูุฌูุงุนุฉ ุงููุณูุญุฉ" โ

๐ซ ุชุญุฐูุฑุงุช ูููุฉ:
โข ูุง ุชุณุชุจุฏู ุฅุฐุง ูุงูุช ุงููููุฉ ูู ุณูุงู ูุฎุชูู ุชูุงูุงู ูุง ุนูุงูุฉ ูู ุจุงูููุงู ุงูููุตูุฏ
โข ุญุงูุธ ุนูู ุงููุนูู ุงูุนุงู ููุฌููุฉ
โข ุชุฃูุฏ ูู ุตุญุฉ ุงูุฅุนุฑุงุจ ูุงูุชุฐููุฑ ูุงูุชุฃููุซ ุจุนุฏ ุงูุงุณุชุจุฏุงู
โข ุฅุฐุง ูุงูุช ุงููููุฉ ุถูู ุงูุชุจุงุณ ุญุฑููุ ุงุณุชุจุฏููุง ุฃูุถุงู

โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
"""
            prompt_parts.append(semantic_instructions)
            error_logger.log_info(f"[Pipeline] Added {len(rule_result.semantic_replacement_rules)} enhanced semantic replacement rules to prompt")
        
        # Add context rules instructions for AI processing
        if hasattr(rule_result, 'ai_instructions') and rule_result.ai_instructions:
            context_instructions = """

๐ ุชุนูููุงุช ุงูุชุญุฑูุฑ ุงูุณูุงูู (ุฅูุฒุงููุฉ):
ูุฐู ุงูุชุนูููุงุช ุชุญุฏุฏ ููููุฉ ุงูุชุนุงูู ูุน ุณูุงูุงุช ูุนููุฉ ูู ุงููุต.
"""
            for i, inst in enumerate(rule_result.ai_instructions, 1):
                instructions_text = inst.get('instructions', '')
                rule_type = inst.get('rule_type', '')
                target_sentiment = inst.get('target_sentiment', 'neutral')
                if instructions_text:
                    sentiment_label = {
                        'positive': '๐ข ุฅูุฌุงุจู',
                        'negative': '๐ด ุณูุจู', 
                        'neutral': 'โช ูุญุงูุฏ'
                    }.get(target_sentiment, 'โช ูุญุงูุฏ')
                    
                    context_instructions += f"""
โโโ ุงูุชุนูููุฉ {i} โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ ๐ {instructions_text}
โ ๐ฏ ุงููุจุฑุฉ ุงููุทููุจุฉ: {sentiment_label}
โ ๐ก ุทุจู ูุฐู ุงูุชุนูููุฉ ุนูู ูู ุฌุฒุก ููุงุณุจ ูู ุงููุต
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
"""
            prompt_parts.append(context_instructions)
            error_logger.log_info(f"[Pipeline] Added {len(rule_result.ai_instructions)} context instructions to prompt")
        
        if preprocessing_result.sentiment.has_offensive:
            offensive_instructions = """

โ๏ธ ุชูุจูู: ุชู ุฑุตุฏ ูุบุฉ ุบูุฑ ููุงุณุจุฉ ูู ุงููุต

๐ ุงูุชุนูููุงุช:
โข ุญููุฏ ุฌููุน ุงูุฃููุงุธ ุงููุณูุฆุฉ ุฃู ุงูุนูููุฉ
โข ุงุณุชุจุฏู ุงูุดุชุงุฆู ูุงูุฅูุงูุงุช ุจูุตู ูุญุงูุฏ
โข ุญุงูุธ ุนูู ุงููุนูู ุฏูู ุงููุบุฉ ุงูุฌุงุฑุญุฉ
โข ุงุฌุนู ุงูุตูุงุบุฉ ููููุฉ ููุญุงูุฏุฉ

๐ ุฃูุซูุฉ ุนูู ุงูุชุญููุฏ:
โข "ุฅุฑูุงุจู/ูุฌุฑู" โ "ูุณูุญ/ูุชูู"
โข "ุนุตุงุจุฉ" โ "ูุฌููุนุฉ"  
โข "ููุจ/ุญูุงุฑ" โ "ุดุฎุต"
โข ุงูุดุชุงุฆู โ ุญุฐููุง ุฃู ุงุณุชุจุฏุงููุง ุจูุตู ูุญุงูุฏ
"""
            prompt_parts.append(offensive_instructions)
        
        if training_examples:
            examples_section = "\n\n๐ ุฃูุซูุฉ ุชุฏุฑูุจูุฉ (ุชุนูู ูู ูุฐู ุงูุฃูุซูุฉ ูุงุชุจุน ููุณ ุงูุฃุณููุจ):"
            for i, example in enumerate(training_examples[:5], 1):
                input_text = example.get('input_text', '')[:200]
                expected_output = example.get('expected_output', '')[:200]
                explanation = example.get('explanation', '')
                example_type = example.get('example_type', 'general')
                
                examples_section += f"\n\n--- ูุซุงู {i} ({example_type}) ---"
                examples_section += f"\nโ๏ธ ุงููุฏุฎู: {input_text}"
                examples_section += f"\nโถ๏ธ ุงููุฎุฑุฌ ุงููุทููุจ: {expected_output}"
                if explanation:
                    examples_section += f"\n๐ก ุงูุณุจุจ: {explanation}"
            
            examples_section += "\n\n--- ุงูุชูุช ุงูุฃูุซูุฉ ---"
            examples_section += "\nุงุชุจุน ููุณ ุงูุฃุณููุจ ูุงูุชุญูููุงุช ูู ุงููุต ุงูุชุงูู."
            prompt_parts.append(examples_section)
            error_logger.log_info(f"[Pipeline] Added {min(len(training_examples), 5)} training examples to prompt")
        
        preserve_format = config.get('preserve_formatting', True)
        output_format = config.get('output_format', 'markdown')
        
        format_instructions = """

๐ ุชุนูููุงุช ุงูุฅุฎุฑุงุฌ ุงูููุงุฆูุฉ:
"""
        if output_format == 'markdown':
            format_instructions += "โข ุงุณุชุฎุฏู ุชูุณูู Markdown ุนูุฏ ุงูุญุงุฌุฉ ููุชูุธูู\n"
            format_instructions += "โข ุงุณุชุฎุฏู ุงูุนูุงููู ูุงูููุงุท ูุชูุถูุญ ุงููุญุชูู\n"
        elif output_format == 'plain':
            format_instructions += "โข ุฃุฎุฑุฌ ูุตุงู ุนุงุฏูุงู ุจุฏูู ุชูุณูู\n"
        
        format_instructions += """โข ูุง ุชุจุฏุฃ ุจูููุฉ 'ููุฎุต' ุฃู 'Summary'
โข ูุง ุชุฐูุฑ ุฃูู ูููุฐุฌ ุฐูุงุก ุงุตุทูุงุนู
โข ูุง ุชุดุฑุญ ูุง ููุช ุจูุ ููุท ุฃุฎุฑุฌ ุงููุต ุงูููุงุฆู

โ ูุงุฆูุฉ ุงูุชุญูู ูุจู ุงูุฅุฎุฑุงุฌ:
โก ูู ุทุจูุช ุฌููุน ููุงุนุฏ ุงูุงุณุชุจุฏุงูุ
โก ูู ุจุญุซุช ุนู ุฌููุน ุตูุบ ุงููููุงุช ุงููุทููุจ ุงุณุชุจุฏุงููุงุ
โก ูู ุญุงูุธุช ุนูู ุณูุงุณุฉ ุงููุต ุจุนุฏ ุงูุงุณุชุจุฏุงูุ
โก ูู ุตุญุฉ ุงูุฅุนุฑุงุจ ูุงูุชุฐููุฑ ูุงูุชุฃููุซ ุณูููุฉุ
โก ูู ุงููุต ูุญุงูุฏ ูููููุ
"""
        prompt_parts.append(format_instructions)
        
        # Add field extraction instructions if fields are provided
        if fields_to_extract:
            # Filter fields that need AI extraction
            ai_fields = [f for f in fields_to_extract 
                        if f.get('field_type') == 'extracted' 
                        and f.get('field_name')]
            
            if ai_fields:
                fields_prompt = ""
                for f in ai_fields:
                    field_name = f.get('field_name', '')
                    instructions = f.get('extraction_instructions', '').strip()
                    fields_prompt += f"\nโข {field_name}: {instructions or 'ุงุณุชุฎุฑุฌ ูู ุงููุต'}"
                
                extraction_instructions = f"""

โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
๐ ุงุณุชุฎุฑุงุฌ ุงูุญููู ุงููุทููุจุฉ:
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

ุจุงูุฅุถุงูุฉ ููุชูุฎูุตุ ุงุณุชุฎุฑุฌ ุงูุญููู ุงูุชุงููุฉ ูู ุงููุต:
{fields_prompt}

โ๏ธ ุชูุณูู ุงูุฅุฎุฑุงุฌ ุงููุทููุจ:
ุฃุฎุฑุฌ ุงูุฑุฏ ุจุงูุชูุณูู ุงูุชุงูู (JSON + ุงูุชูุฎูุต):

```json
{{
  "ุงูุชูุฎูุต": "ุงููุต ุงูููุฎุต ููุง",
{chr(10).join([f'  "{f.get("field_name")}": "ุงููููุฉ ุงููุณุชุฎุฑุฌุฉ"' + (',' if i < len(ai_fields)-1 else '') for i, f in enumerate(ai_fields)])}
}}
```

ููุงุญุธุงุช:
- ุถุน ุงูุชูุฎูุต ูู ุญูู "ุงูุชูุฎูุต"
- ุฅุฐุง ูู ุชุฌุฏ ูููุฉ ูุญูู ูุงุ ุงุชุฑูู ูุงุฑุบุงู ""
- ุฃุฎุฑุฌ JSON ุตุญูุญ ููุท ุจุฏูู ุฃู ูุต ุฅุถุงูู
"""
                prompt_parts.append(extraction_instructions)
                
                prompt_parts.append(f"""
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
๐ ุงููุต ุงููุทููุจ ูุนุงูุฌุชู:
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

{text}

โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โฌ๏ธ ุฃุฎุฑุฌ JSON ููุท ุจุงูุชูุณูู ุงููุทููุจ ุฃุนูุงู:
""")
            else:
                # No AI fields, regular output
                prompt_parts.append(f"""
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
๐ ุงููุต ุงููุทููุจ ูุนุงูุฌุชู:
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

{text}

โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โฌ๏ธ ุฃุฎุฑุฌ ุงููุต ุงููุนุงูุฌ ููุท (ุจุฏูู ุดุฑุญ ุฃู ุชุนูููุงุช):
""")
        else:
            prompt_parts.append(f"""
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
๐ ุงููุต ุงููุทููุจ ูุนุงูุฌุชู:
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

{text}

โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โฌ๏ธ ุฃุฎุฑุฌ ุงููุต ุงููุนุงูุฌ ููุท (ุจุฏูู ุดุฑุญ ุฃู ุชุนูููุงุช):
""")
        
        return "\n".join(prompt_parts)
    
    def _parse_combined_response(
        self,
        response: str,
        fields_to_extract: List[Dict[str, Any]]
    ) -> Tuple[str, Dict[str, Any]]:
        """
        Parse combined AI response containing both summary and extracted fields
        Returns tuple of (summary_text, extracted_fields)
        """
        import json
        import re
        
        extracted = {}
        summary = response  # Default to full response if parsing fails
        
        try:
            # Try to find JSON in response
            json_match = re.search(r'\{[\s\S]*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                parsed = json.loads(json_str)
                
                # Extract summary
                summary = parsed.get('ุงูุชูุฎูุต', '') or parsed.get('summary', '') or ''
                
                # Extract other fields with normalization
                for k, v in parsed.items():
                    if k in ['ุงูุชูุฎูุต', 'summary']: continue
                    
                    val = str(v).strip() if v is not None else ""
                    # Store original key
                    extracted[k] = val
                    
                    # Store normalized key (remove underscores and handle Arabic variants)
                    norm_k = k.replace('_', '').replace(' ', '').strip()
                    extracted[norm_k] = val
                    
                    # Handle specific Arabic field mappings
                    if 'ูุญุงูุธู' in norm_k or 'ูุญุงูุธุฉ' in norm_k or 'governorate' in norm_k:
                        extracted['ุงููุญุงูุธุฉ'] = val
                        extracted['ุงููุญุงูุธู'] = val
                        extracted['governorate'] = val
                        
                    if 'ูุตุฏุฑ' in norm_k or 'source' in norm_k:
                        extracted['ุงููุตุฏุฑ'] = val
                        extracted['source'] = val
                        
                    if 'ุชุตููู' in norm_k or 'classification' in norm_k or 'category' in norm_k:
                        extracted['ุงูุชุตููู'] = val
                        extracted['category'] = val

                error_logger.log_info(f"[Pipeline] โ Parsed combined response | Summary: {len(summary)} chars | Fields: {len(extracted)}")
        except json.JSONDecodeError as e:
            error_logger.log_warning(f"[Pipeline] โ๏ธ Failed to parse JSON from response: {str(e)}")
            summary = response
        except Exception as e:
            error_logger.log_warning(f"[Pipeline] โ๏ธ Error parsing combined response: {str(e)}")
            summary = response
        
        return (summary, extracted)
    
    async def _get_training_examples(self, task_id: int) -> List[Dict]:
        """Get training examples for few-shot learning"""
        try:
            examples = await db.get_training_examples(task_id=task_id)
            if examples:
                active_examples = [e for e in examples if e.get('is_active', True)]
                
                # Increment use_count for each example used
                for example in active_examples[:5]:  # Only for the ones we'll actually use
                    try:
                        await db.increment_example_use_count(example['id'])
                    except Exception as count_err:
                        error_logger.log_info(f"[Pipeline] Failed to increment use_count for example {example['id']}: {str(count_err)}")
                
                return active_examples
        except Exception as e:
            error_logger.log_info(f"[Pipeline] No training examples for task {task_id}: {str(e)}")
        return []
    
    async def _get_config(self, task_id: int) -> Dict[str, Any]:
        """Get processing configuration for task"""
        try:
            config = await db.get_processing_config(task_id)
            if config:
                return config
        except Exception as e:
            error_logger.log_info(f"[Pipeline] No custom config for task {task_id}, using defaults")
        
        return {
            'enable_entity_extraction': True,
            'enable_sentiment_analysis': True,
            'enable_keyword_detection': True,
            'enable_output_validation': True,
            'enable_rule_verification': True,
            'preserve_formatting': True,
            'output_format': 'markdown',
            'temperature': '0.7',
            'quality_level': 'balanced'
        }
    
    def _empty_result(self, text: str) -> PipelineResult:
        """Return empty result for empty text"""
        return PipelineResult(
            original_text=text or "",
            final_text=text or "",
            stages=[],
            success=True
        )
    
    async def process_video_summary(
        self,
        transcript: str,
        task_id: int,
        provider: str,
        model: str,
        config: Optional[Dict[str, Any]] = None
    ) -> PipelineResult:
        """
        Process video transcript with specialized video summarization
        """
        video_system_prompt = """ุฃูุช ูุญุฑุฑ ููุฏูู ูุญุชุฑู ูุชุฎุตุต ูู ุชูุฎูุต ูุญุชูู ุงูููุฏูููุงุช.
ูููุชู: ุชูุฎูุต ูุญุชูู ุงูููุฏูู ุจุดูู ููุฌุฒ ูุฏููู ูุน:
- ุงูุชุฑููุฒ ุนูู ุงูููุงุท ุงูุฑุฆูุณูุฉ
- ุฐูุฑ ุงููุชุญุฏุซูู ุงูุฑุฆูุณููู ุฅู ูุฌุฏูุง
- ุชูุฎูุต ุงูุฃุญุฏุงุซ ุงููููุฉ ุจุชุฑุชูุจ ุฒููู
- ุงูุญูุงุธ ุนูู ุงูุณูุงู ุงูุนุงู ููููุฏูู"""
        
        video_rules = await db.get_task_rules(task_id)
        video_rules = [r for r in video_rules if r.get('type') == 'video_summarize' and r.get('is_active')]
        
        return await self.process(
            text=transcript,
            task_id=task_id,
            provider=provider,
            model=model,
            system_prompt=video_system_prompt,
            custom_rules=video_rules,
            config=config
        )
    
    async def process_audio_summary(
        self,
        transcript: str,
        task_id: int,
        provider: str,
        model: str,
        config: Optional[Dict[str, Any]] = None
    ) -> PipelineResult:
        """
        Process audio transcript with specialized audio summarization
        Applies AUDIO-SPECIFIC summarization rules (type='audio_summarize')
        Falls back to general summarization rules if no audio-specific rules exist
        """
        transcript_length = len(transcript)
        
        if transcript_length < 200:
            target_length = "50-100 ูููุฉ"
        elif transcript_length < 500:
            target_length = "100-150 ูููุฉ"
        elif transcript_length < 1000:
            target_length = "150-250 ูููุฉ"
        elif transcript_length < 3000:
            target_length = "250-400 ูููุฉ"
        else:
            target_length = "400-600 ูููุฉ"
        
        audio_system_prompt = f"""ุฃูุช ูุญุฑุฑ ุตูุชู ูุญุชุฑู ูุชุฎุตุต ูู ุชูุฎูุต ูุญุชูู ุงูููุงุทุน ุงูุตูุชูุฉ ูุงูุฑุณุงุฆู ุงูุตูุชูุฉ.

โ๏ธ ุชุนูููุงุช ุตุงุฑูุฉ - ูุฌุจ ุงุชุจุงุนูุง ุจุฏูุฉ:

1. ๐ ูู ุจุฅูุชุงุฌ ููุฎุต ูุฎุชุตุฑ ูููุฌุฒ ูููุต ุงููููุฑูุบ (ูููุณ ุงููุต ุงููุงูู)
2. ๐ ุงูุทูู ุงููุทููุจ ููููุฎุต: {target_length} (ุงููุต ุงูุฃุตูู {transcript_length} ุญุฑู)
3. ๐ฏ ุงุณุชุฎุฑุฌ ุงูููุงุท ุงูุฑุฆูุณูุฉ ูุงูุฃููุงุฑ ุงููููุฉ ููุท
4. โ ูุง ุชูุนูุฏ ูุชุงุจุฉ ุงููุต ุงููุงูู - ูู ุจุชูุฎูุตู ููุท
5. โ ุฃุนุฏ ุงูููุฎุต ูุจุงุดุฑุฉ ุจุฏูู ููุฏูุงุช ุฃู ุนุจุงุฑุงุช ูุซู "ุฅููู ุงูููุฎุต"

๐ ูุนุงููุฑ ุงูุชูุฎูุต ุงูุฌูุฏ:
- ุงูุชุฑููุฒ ุนูู ุงููุนูููุงุช ุงูุฃุณุงุณูุฉ ูุงูุฌููุฑูุฉ
- ุญุฐู ุงูุชูุฑุงุฑุงุช ูุงูุญุดู ูุงูุชูุงุตูู ุงูุซุงูููุฉ
- ุงุณุชุฎุฏุงู ุฌูู ูุตูุฑุฉ ููุจุงุดุฑุฉ
- ุฐูุฑ ุงูุฃุณูุงุก ูุงูุฃุฑูุงู ูุงูุชูุงุฑูุฎ ุงููููุฉ
- ุงูุญูุงุธ ุนูู ุงูุณูุงู ุงูุนุงู ูุงููุนูู ุงูุฃุตูู

๐ซ ุชุฌูุจ:
- ุฅุนุงุฏุฉ ุงููุต ููุง ูู
- ุฅุถุงูุฉ ูุนูููุงุช ุบูุฑ ููุฌูุฏุฉ ูู ุงููุต ุงูุฃุตูู
- ุงุณุชุฎุฏุงู ุนุจุงุฑุงุช ุทูููุฉ ููุนูุฏุฉ
- ุฐูุฑ ุชูุงุตูู ุบูุฑ ุถุฑูุฑูุฉ"""
        
        # First try to get audio-specific rules
        all_rules = await db.get_task_rules(task_id)
        audio_rules = [r for r in all_rules if r.get('type') == 'audio_summarize' and r.get('is_active')]
        
        # If no audio-specific rules exist, fall back to general summarization rules
        if not audio_rules:
            error_logger.log_info(f"[Pipeline] No audio-specific rules found, falling back to general summarization rules")
            audio_rules = [r for r in all_rules if r.get('type') == 'summarize' and r.get('is_active')]
        
        if audio_rules:
            error_logger.log_info(f"[Pipeline] ๐๏ธ Loaded {len(audio_rules)} AUDIO-SPECIFIC summarization rules for task {task_id}")
        
        return await self.process(
            text=transcript,
            task_id=task_id,
            provider=provider,
            model=model,
            system_prompt=audio_system_prompt,
            custom_rules=audio_rules,
            config=config
        )

ai_pipeline = AIPipeline()
